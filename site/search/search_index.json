{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>A bunch of notes on Python, Javascript, SQL etc</p>"},{"location":"#useful-linkswebsites","title":"Useful links/websites","text":""},{"location":"#dev-environment","title":"Dev environment","text":"<p>poetry, pyenv, pre-commmit, VS code etc setup - Python Best Practices for a New Project</p> <p>FastAPI template</p>"},{"location":"#text-extraction-librariesservices","title":"Text extraction libraries/services","text":"<p>Docling</p> <p>PyMuPDF4</p> <p>PyMuPDF4LLM</p> <p>Adobe PDF Extract API</p>"},{"location":"SQL/","title":"SQL","text":""},{"location":"SQL/#postgres","title":"Postgres","text":""},{"location":"SQL/#tools-and-cheatsheet","title":"Tools and cheatsheet","text":"<p>Online playground: DB Fiddle GUI tool: TablePlus Postgres.app (Mac) Cheatsheet PostgreSQL Wiki</p>"},{"location":"SQL/#enable-uuid-generation","title":"Enable uuid generation","text":"<pre><code>CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n</code></pre>"},{"location":"SQL/#create-table-with-foreign-key-constraintsreferences","title":"Create table with foreign key constraints/references","text":"<pre><code>CREATE TABLE departments (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nname VARCHAR(255) NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\nCREATE TABLE employees (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nemail VARCHAR(255) UNIQUE NOT NULL,\nfirst_name VARCHAR(255) NOT NULL,\nlast_name VARCHAR(255) NOT NULL,\ndepartment_id uuid NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\nFOREIGN KEY (department_id) REFERENCES departments(id) ON DELETE CASCADE\n);\n</code></pre> <p>or</p> <pre><code>CREATE TABLE departments (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nname VARCHAR(255) NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\nCREATE TABLE employees (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nemail VARCHAR(255) UNIQUE NOT NULL,\nfirst_name VARCHAR(255) NOT NULL,\nlast_name VARCHAR(255) NOT NULL,\ndepartment_id uuid NOT NULL REFERENCES departments(id) ON DELETE CASCADE,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\n</code></pre>"},{"location":"SQL/#insert-values-to-table","title":"Insert values to table","text":"<pre><code>INSERT INTO\ndepartments(name)\nVALUES\n('HR'),\n('Finance');\nINSERT INTO\nemployees(email, first_name, last_name, department_id)\nVALUES\n('mary@test_co.com', 'Mary', 'Smith', (SELECT id from departments WHERE name='Finance')),\n('dave@test_co.com', 'Dave', 'Cole', (SELECT id from departments WHERE name='Finance')),\n('jane@test_co.com', 'Jane', 'Hills', (SELECT id from departments WHERE name='Finance')),\n('john@test_co.com', 'John', 'Doe', (SELECT id from departments WHERE name='HR'));\n</code></pre>"},{"location":"SQL/#simple-audit-table","title":"Simple audit table","text":"<pre><code>CREATE OR REPLACE FUNCTION employees_audit_func()\nRETURNS TRIGGER\nAS $employees_audit$\nBEGIN\nif (TG_OP = 'UPDATE') THEN\nINSERT INTO employees_audit\nSELECT\nuuid_generate_v4(),\n'UPDATE',\nnow(),\nNEW.*;\nelsif (TG_OP = 'INSERT') THEN\nINSERT INTO employees_audit\nSELECT\nuuid_generate_v4(),\n'INSERT',\nnow(),\nNEW.*;\nelsif (TG_OP = 'DELETE') THEN\nINSERT INTO employees_audit\nSELECT\nuuid_generate_v4(),\n'DELETE',\nnow(),\nOLD.*;\nEND IF;\nRETURN NULL;\nEND;\n$employees_audit$\nLANGUAGE plpgsql;\nCREATE TRIGGER employees_audit_trigger\nAFTER INSERT OR UPDATE OR DELETE ON employees FOR EACH ROW\nEXECUTE PROCEDURE employees_audit_func();\n</code></pre> <p>or only tracking certain columns on update (e.g all columns except created_at and updated_at)</p> <pre><code>CREATE TRIGGER employees_audit_update_selective_trigger\nAFTER UPDATE ON employees FOR EACH ROW\nWHEN ( (to_jsonb(OLD.*) - 'updated_at' - 'created_at') IS DISTINCT FROM  (to_jsonb(NEW.*) - 'updated_at' - 'created_at') )\nEXECUTE PROCEDURE employees_audit_func();\n</code></pre>"},{"location":"SQL/#updated_at-timestamp-trigger","title":"updated_at timestamp trigger","text":"<pre><code>CREATE OR REPLACE FUNCTION trigger_set_timestamp()\nRETURNS TRIGGER AS $$\nBEGIN\nNEW.updated_at = NOW();\nRETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\nCREATE TRIGGER set_timestamp_employees\nBEFORE UPDATE ON employees\nFOR EACH ROW\nEXECUTE PROCEDURE trigger_set_timestamp();\n</code></pre>"},{"location":"SQL/#add-column-and-foreign-key-constraint-to-existing-table","title":"Add column and foreign key constraint to existing table","text":"<pre><code>ALTER TABLE employees\nADD COLUMN department_id uuid DEFAULT NULL, -- Set to NOT NULL once data is populated\nADD CONSTRAINT employee_id_department_id FOREIGN KEY (department_id) REFERENCES departments(id);\n</code></pre> <p>Set column to <code>NOT NULL</code></p> <pre><code>ALTER TABLE employees ALTER COLUMN department_id SET NOT NULL;\n</code></pre>"},{"location":"SQL/#alter-table-column-type-with-casting","title":"Alter table column type (with casting)","text":"<pre><code>CREATE TABLE employees (\n...\nemployment_start_year VARCHAR(4) NOT NULL\n...\n);\nALTER TABLE employees ALTER employment_start_year TYPE INT\nUSING employment_start_year::INTEGER;\n</code></pre>"},{"location":"SQL/#update-values-with-unnest","title":"Update values with unnest","text":"<pre><code>UPDATE\nemployees\nSET\nemail = data_table.email\nFROM (\nSELECT\nunnest(ARRAY ['Mary', 'John', 'Dawn']) AS first_name,\nunnest(ARRAY ['Smith', 'Doe', 'Carter']) AS last_name) AS data_table\nWHERE\nemployees.email = data_table.email;\n</code></pre>"},{"location":"SQL/#json-aggregation-and-functions","title":"JSON aggregation and functions","text":"<pre><code>SELECT\ndept.name AS department,\njsonb_agg(\nDISTINCT jsonb_build_object (  -- DISTINCT to remove dupes\n'employee_id', e.id,\n'email', e.email,\n'first_name', e.first_name,\n'last_name', e.last_name\n)\n) AS department_staff\nFROM\nemployees e\nJOIN departments dept ON e.department_id = dept.id\nGROUP BY\nd.name, d.id;\n</code></pre> <p>Returns:</p> department department_staff Finance [{\"email\": \"dave@test_co.com\", \"last_name\": \"Cole\", \"first_name\": \"Dave\", \"employee_id\": \"b26339e1-af22-4752-852e-cb51f342bb10\"}, {\"email\": \"jane@test_co.com\", \"last_name\": \"Hills\", \"first_name\": \"Jane\", \"employee_id\": \"710b5de9-f248-49d8-9846-57b31ed143b2\"}, {\"email\": \"mary@test_co.com\", \"last_name\": \"Smith\", \"first_name\": \"Mary\", \"employee_id\": \"e2044159-0576-4a3a-9fe5-9af24bf66102\"}] HR [{\"email\": \"john@test_co.com\", \"last_name\": \"Doe\", \"first_name\": \"John\", \"employee_id\": \"bcdadb45-970e-4d87-8b8a-fab1ccfeddd4\"}] <pre><code>SELECT\nd.name,\njsonb_object_agg(e.email, (concat_ws(' ', e.first_name, e.last_name))) AS department_staff\nFROM\nemployees e\nJOIN departments d ON e.department_id = d.id\nGROUP BY\nd.name,\nd.id;\n</code></pre> <p>Returns:</p> department department_staff Finance {\"dave@test_co.com\": \"Dave Cole\", \"jane@test_co.com\": \"Jane Hills\", \"mary@test_co.com\": \"Mary Smith\"} HR {\"john@test_co.com\": \"John Doe\"}"},{"location":"SQL/#get-week-number-of-a-date","title":"Get week number of a date","text":"<p>N.B. With last day of week is Saturady i.e. new week begins on Sunday</p> <pre><code>CREATE OR REPLACE FUNCTION get_week_number_for_date (date_input date) RETURNS int LANGUAGE plpgsql AS $$ BEGIN RETURN (\n(\n$1 - DATE_TRUNC('year', $1)::date\n) + DATE_PART('isodow', DATE_TRUNC('year', $1))\n)::int / 7 + CASE\nWHEN DATE_PART('isodow', DATE_TRUNC('year', $1)) = 7 THEN 0\nELSE 1\nEND;\nEND;\n$$;\nget_week_number_for_date('2020-01-01');\n</code></pre>"},{"location":"SQL/#count-business-days-between-2-dates","title":"Count business days between 2 dates","text":"<pre><code>CREATE OR REPLACE FUNCTION business_days_count (from_date date, to_date date)\nRETURNS int\nLANGUAGE plpgsql\nAS $$\nBEGIN\nRETURN (SELECT\ncount(d::date) AS d\nFROM\ngenerate_series(from_date, to_date, '1 day'::interval) d\nWHERE\nextract('dow' FROM d)\nNOT in(0, 6));\nEND;\n$$;\n</code></pre>"},{"location":"SQL/#generate_series","title":"GENERATE_SERIES","text":"<p><code>GENERATE_SERIES</code> is pretty handy when creating time-series dataset</p> <pre><code>SELECT * FROM GENERATE_SERIES(2019, 2021, 1) AS \"year\", GENERATE_SERIES(1, 12, 1) AS \"month\";\n</code></pre> <p>Will generate a year-month table</p> year month 2019 1 2019 2 ... ... 2019 12 2020 1 2020 2 ... ... 2020 12 2021 1 2021 2 ... ... 2021 12 <p>...and can also produce a range of dates/time</p> <pre><code>SELECT * FROM generate_series('2022-01-01','2022-01-02', INTERVAL '1 hour');\n</code></pre> generate_series 2022-01-01T00:00:00.000Z 2022-01-01T01:00:00.000Z 2022-01-01T02:00:00.000Z 2022-01-01T03:00:00.000Z 2022-01-01T04:00:00.000Z 2022-01-01T05:00:00.000Z 2022-01-01T06:00:00.000Z 2022-01-01T07:00:00.000Z 2022-01-01T08:00:00.000Z 2022-01-01T09:00:00.000Z 2022-01-01T10:00:00.000Z 2022-01-01T11:00:00.000Z 2022-01-01T12:00:00.000Z 2022-01-01T13:00:00.000Z 2022-01-01T14:00:00.000Z 2022-01-01T15:00:00.000Z 2022-01-01T16:00:00.000Z 2022-01-01T17:00:00.000Z 2022-01-01T18:00:00.000Z 2022-01-01T19:00:00.000Z 2022-01-01T20:00:00.000Z 2022-01-01T21:00:00.000Z 2022-01-01T22:00:00.000Z 2022-01-01T23:00:00.000Z 2022-01-02T00:00:00.000Z <p>... and add some random generated data</p> <pre><code>SELECT random() as rand_figures, *\nFROM generate_series('2022-01-01','2022-01-02', INTERVAL '1 hour');\n</code></pre> rand_figures generate_series 0.203633273951709 2022-01-01T00:00:00.000Z 0.571097886189818 2022-01-01T01:00:00.000Z 0.629665858577937 2022-01-01T02:00:00.000Z 0.0612306422553957 2022-01-01T03:00:00.000Z 0.431237444281578 2022-01-01T04:00:00.000Z 0.229508123826236 2022-01-01T05:00:00.000Z 0.867487183306366 2022-01-01T06:00:00.000Z 0.758365222252905 2022-01-01T07:00:00.000Z 0.155569355469197 2022-01-01T08:00:00.000Z 0.786357307806611 2022-01-01T09:00:00.000Z 0.284404154401273 2022-01-01T10:00:00.000Z 0.367461221758276 2022-01-01T11:00:00.000Z 0.754724379163235 2022-01-01T12:00:00.000Z 0.0396546637639403 2022-01-01T13:00:00.000Z 0.276610609609634 2022-01-01T14:00:00.000Z 0.96564608765766 2022-01-01T15:00:00.000Z 0.127415937371552 2022-01-01T16:00:00.000Z 0.110610570758581 2022-01-01T17:00:00.000Z 0.764237959869206 2022-01-01T18:00:00.000Z 0.24844411527738 2022-01-01T19:00:00.000Z 0.0547867906279862 2022-01-01T20:00:00.000Z 0.977096977643669 2022-01-01T21:00:00.000Z 0.677903080359101 2022-01-01T22:00:00.000Z 0.173856796696782 2022-01-01T23:00:00.000Z 0.896873883903027 2022-01-02T00:00:00.000Z"},{"location":"SQL/#covert-datetimedatetimestamp-to-string","title":"Covert datetime/date/timestamp to string","text":"<pre><code>SELECT TO_CHAR(TIMESTAMP '2023-01-01 05:00:00', 'YYYY-MM-DD');\n</code></pre>"},{"location":"SQL/#postgres-upsert-insert-intoon-conflict-do-nothingupdate-set","title":"Postgres upsert (INSERT INTO...ON CONFLICT... DO NOTHING/UPDATE SET...)","text":"<p>DO NOTHING (<code>email</code> is unique)</p> <pre><code>INSERT INTO users (email, username, tel)\nVALUES\n('user_one@email.com', 'i_am_user_one', '0123456789'),\n('user_two@email.com', 'i_am_user_two', '9876543210')\nON CONFLICT (email)\nDO NOTHING;\n</code></pre> <p>DO UPDATE SET (<code>email</code> is unique)</p> <pre><code>INSERT INTO users (email, username, tel)\nVALUES\n('user_one@email.com', 'i_am_user_one', '0123456789'),\n('user_two@email.com', 'i_am_user_two', '9876543210')\nON CONFLICT (name)\nDO UPDATE SET username = EXCLUDED.username, tel = EXCLUDED.tel;\n</code></pre>"},{"location":"SQL/#closure-table","title":"Closure table","text":"<pre><code>CREATE TABLE companies (\nid uuid PRIMARY KEY NOT NULL DEFAULT uuid_generate_v4(),\nname VARCHAR UNIQUE NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE\n);\nCREATE TABLE departments (\nid uuid PRIMARY KEY NOT NULL DEFAULT uuid_generate_v4(),\nname VARCHAR NOT NULL,\ncompany_id uuid NOT NULL REFERENCES companies(id) ON DELETE CASCADE,\ncode VARCHAR(255) DEFAULT NULL::character varying,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE\n);\nCREATE UNIQUE INDEX departments_name_key ON public.departments USING btree (name);\nCREATE UNIQUE INDEX departments_unique ON public.departments USING btree (name, company_id);\nCREATE TABLE IF NOT EXISTS closure_tbl (\nancestor_id UUID NOT NULL,\ndescendant_id UUID NOT NULL,\ndepth INT NOT NULL,\nPRIMARY KEY (ancestor_id, descendant_id),\nFOREIGN KEY (ancestor_id) REFERENCES departments(id),\nFOREIGN KEY (descendant_id) REFERENCES departments(id)\n);\nCREATE INDEX IF NOT EXISTS closure_tbl_ancestor_depth_idx ON closure_tbl (ancestor_id, depth);\nCREATE INDEX IF NOT EXISTS closure_tbl_descendant_idx ON closure_tbl (descendant_id);\n-- Get all departments as tree/descendants with top level (main) as row entry\nCREATE OR REPLACE FUNCTION get_departments_tree()\nRETURNS TABLE (\nid UUID,\nname VARCHAR,\ncompany_id UUID,\ncompany_name VARCHAR,\ndescendants JSONB\n) AS $$\nBEGIN\nRETURN QUERY\nWITH RECURSIVE descendants AS (\nSELECT\ncd.id,\ncd.name,\ncd.company_id,\ncd.company_name,\nct.depth,\nct.ancestor_id AS root_id,\nct.ancestor_id AS parent_id\nFROM closure_tbl ct\nINNER JOIN company_departments cd ON cd.id = ct.descendant_id\nWHERE ct.depth = 1\nUNION ALL\nSELECT\nchild.id,\nchild.name,\nchild.company_id,\nchild.company_name,\nparent.depth + 1 AS depth,\nparent.root_id,\nparent.id AS parent_id\nFROM descendants parent\nINNER JOIN closure_tbl ct ON ct.ancestor_id = parent.id AND ct.depth = 1\nINNER JOIN company_departments child ON child.id = ct.descendant_id\n),\ncompany_departments AS (\nSELECT\ndepts.id,\ndepts.name,\ndepts.company_id,\nc.name AS company_name\nFROM departments depts\nINNER JOIN companies c ON c.id = depts.company_id\n),\njson_descendants AS (\nSELECT\nd.id,\nd.name,\nd.company_id,\nd.company_name,\nd.parent_id,\nd.root_id,\njsonb_build_object(\n'id', d.id,\n'name', d.name,\n'company_id', d.company_id,\n'company_name', d.company_name,\n'children', '[]'::jsonb\n) AS children\nFROM descendants d\n),\nnested_json_descendants AS (\nSELECT\njd.id,\njd.parent_id,\njd.root_id,\njd.company_id,\njd.company_name,\njd.children || jsonb_build_object(\n'children',\nCOALESCE(\njsonb_agg(jd2.children ORDER BY jd2.name) FILTER (WHERE jd2.children IS NOT NULL),\n'[]'::jsonb)\n) AS children\nFROM json_descendants jd\nLEFT JOIN json_descendants jd2 ON jd2.parent_id = jd.id\nGROUP BY jd.id, jd.company_id, jd.company_name, jd.parent_id, jd.children, jd.root_id\n),\ngrouped_descendants AS (\nSELECT\nnjd.root_id,\nnjd.company_id,\nnjd.company_name,\njsonb_agg(njd.children ORDER BY njd.children -&gt;&gt; 'name') AS children\nFROM nested_json_descendants njd\nWHERE njd.parent_id = njd.root_id\nGROUP BY njd.root_id, njd.company_id, njd.company_name\n)\nSELECT\ncd.id,\ncd.name,\ncd.company_id,\ncd.company_name,\nCOALESCE(gd.children, '[]'::jsonb) AS \"descendants\"\nFROM company_departments cd\nINNER JOIN closure_tbl ct ON cd.id = ct.descendant_id AND ct.depth = 0\nLEFT JOIN grouped_descendants gd ON gd.root_id = cd.id\nORDER BY 4, 2;\nEND;\n$$ LANGUAGE plpgsql STABLE;\n-- GET by company ID\nCREATE OR REPLACE FUNCTION get_departments_tree_by_company(cid uuid)\nRETURNS TABLE (\nid UUID,\nname VARCHAR,\ndescendants JSONB\n) AS $$\nBEGIN\nRETURN QUERY\nWITH RECURSIVE descendants AS (\nSELECT\ncd.id,\ncd.name,\nct.depth,\nct.ancestor_id AS root_id,\nct.ancestor_id AS parent_id\nFROM closure_tbl ct\nINNER JOIN company_departments cd ON cd.id = ct.descendant_id\nWHERE ct.depth = 1\nUNION ALL\nSELECT\nchild.id,\nchild.name,\nparent.depth + 1 AS depth,\nparent.root_id,\nparent.id AS parent_id\nFROM descendants parent\nINNER JOIN closure_tbl ct ON ct.ancestor_id = parent.id AND ct.depth = 1\nINNER JOIN company_departments child ON child.id = ct.descendant_id\n),\ncompany_departments AS (\nSELECT\ndepts.id,\ndepts.name\nFROM departments depts\nINNER JOIN companies c ON c.id = depts.company_id\nWHERE c.id = cid\n),\njson_descendants AS (\nSELECT\nd.id,\nd.name,\nd.parent_id,\nd.root_id,\njsonb_build_object(\n'id', d.id,\n'name', d.name,\n'children', '[]'::jsonb\n) AS children\nFROM descendants d\n),\nnested_json_descendants AS (\nSELECT\njd.id,\njd.parent_id,\njd.root_id,\njd.children || jsonb_build_object(\n'children',\nCOALESCE(\njsonb_agg(jd2.children ORDER BY jd2.name) FILTER (WHERE jd2.children IS NOT NULL),\n'[]'::jsonb)\n) AS children\nFROM json_descendants jd\nLEFT JOIN json_descendants jd2 ON jd2.parent_id = jd.id\nGROUP BY jd.id, jd.parent_id, jd.children, jd.root_id\n),\ngrouped_descendants AS (\nSELECT\nnjd.root_id,\njsonb_agg(njd.children ORDER BY njd.children -&gt;&gt; 'name') AS children\nFROM nested_json_descendants njd\nWHERE njd.parent_id = njd.root_id\nGROUP BY njd.root_id\n)\nSELECT\ncd.id,\ncd.name,\nCOALESCE(gd.children, '[]'::jsonb) AS \"descendants\"\nFROM company_departments cd\nINNER JOIN closure_tbl ct ON cd.id = ct.descendant_id AND ct.depth = 0\nLEFT JOIN grouped_descendants gd ON gd.root_id = cd.id\nORDER BY 2;\nEND;\n$$ LANGUAGE plpgsql STABLE;\n-- Get all departments as tree/descendants and group by company\nCREATE OR REPLACE FUNCTION get_departments_tree_grouped_by_company_name()\nRETURNS TABLE (\ncompany_id UUID,\ncompany_name VARCHAR,\ndepatments JSONB\n) AS $$\nBEGIN\nRETURN QUERY\nWITH RECURSIVE descendants AS (\nSELECT\ncd.id,\ncd.name,\ncd.company_id,\ncd.company_name,\nct.depth,\nct.ancestor_id AS root_id,\nct.ancestor_id AS parent_id\nFROM closure_tbl ct\nINNER JOIN company_departments cd ON cd.id = ct.descendant_id\nWHERE ct.depth = 1\nUNION ALL\nSELECT\nchild.id,\nchild.name,\nchild.company_id,\nchild.company_name,\nparent.depth + 1 AS depth,\nparent.root_id,\nparent.id AS parent_id\nFROM descendants parent\nINNER JOIN closure_tbl ct ON ct.ancestor_id = parent.id AND ct.depth = 1\nINNER JOIN company_departments child ON child.id = ct.descendant_id\n),\ncompany_departments AS (\nSELECT\ndepts.id,\ndepts.name,\ndepts.company_id,\nc.name AS company_name\nFROM departments depts\nINNER JOIN companies c ON c.id = depts.company_id\n),\njson_descendants AS (\nSELECT\nd.id,\nd.name,\nd.company_id,\nd.company_name,\nd.parent_id,\nd.root_id,\njsonb_build_object(\n'id', d.id,\n'name', d.name,\n'company_id', d.company_id,\n'company_name', d.company_name,\n'children', '[]'::jsonb\n) AS children\nFROM descendants d\n),\nnested_json_descendants AS (\nSELECT\njd.id,\njd.parent_id,\njd.root_id,\njd.company_id,\njd.company_name,\njd.children || jsonb_build_object(\n'children',\nCOALESCE(\njsonb_agg(jd2.children ORDER BY jd2.name) FILTER (WHERE jd2.children IS NOT NULL),\n'[]'::jsonb)\n) AS children\nFROM json_descendants jd\nLEFT JOIN json_descendants jd2 ON jd2.parent_id = jd.id\nGROUP BY jd.id, jd.company_id, jd.company_name, jd.parent_id, jd.children, jd.root_id\n),\ngrouped_descendants AS (\nSELECT\nnjd.root_id,\nnjd.company_id,\nnjd.company_name,\njsonb_agg(njd.children ORDER BY njd.children -&gt;&gt; 'name') AS children\nFROM nested_json_descendants njd\nWHERE njd.parent_id = njd.root_id\nGROUP BY njd.root_id, njd.company_id, njd.company_name\n)\nSELECT\ncd.company_id,\ncd.company_name,\njsonb_agg(\njsonb_build_object(\n'id', cd.id,\n'departments', cd.name,\n'descendants', COALESCE(gd.children, '[]'::jsonb)\n) ORDER BY cd.name\n) AS departments\nFROM company_departments cd\nINNER JOIN closure_tbl ct ON cd.id = ct.descendant_id AND ct.depth = 0\nLEFT JOIN grouped_descendants gd ON gd.root_id = cd.id\nGROUP BY cd.company_name, cd.company_id\nORDER BY cd.company_name;\nEND;\n$$ LANGUAGE plpgsql STABLE;\n</code></pre>"},{"location":"airflow/","title":"Airflow","text":""},{"location":"airflow/#tutorials","title":"Tutorials","text":"<p>marclamberti</p>"},{"location":"airflow/#run-with-docker","title":"Run with Docker","text":"<p>Airflow Installlation with docker images</p>"},{"location":"airflow/#docker-compose-file-w-postgres-mongodb-rabbitmq-celery","title":"docker compose file (w/ postgres + mongodb + rabbitmq + celery)","text":"<pre><code># Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.\n#\n# WARNING: This configuration is for local development. Do not use it in a production deployment.\n#\n# This configuration supports basic configuration using environment variables or an .env file\n# The following variables are supported:\n#\n# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.\n#                                Default: apache/airflow:3.0.1\n# AIRFLOW_UID                  - User ID in Airflow containers\n#                                Default: 50000\n# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.\n#                                Default: .\n# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode\n#\n# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).\n#                                Default: airflow\n# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).\n#                                Default: airflow\n# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.\n#                                Use this option ONLY for quick checks. Installing requirements at container\n#                                startup is done EVERY TIME the service is started.\n#                                A better way is to build a custom image or extend the official image\n#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.\n#                                Default: ''\n#\n# Feel free to modify this file to suit your needs.\n---\nx-airflow-common: &amp;airflow-common\n  # In order to add custom dependencies or upgrade provider distributions you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.1}\n  # build: .\n  environment: &amp;airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    # AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CELERY__BROKER_URL: amqp://:@rabbitmq:5672/airflow-vhost\n    AIRFLOW__CORE__FERNET_KEY: \"\"\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \"true\"\n    AIRFLOW__CORE__LOAD_EXAMPLES: \"true\"\n    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: \"http://airflow-apiserver:8080/execution/\"\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \"true\"\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n    # The following line can be used to set a custom config file, stored in the local config folder\n    AIRFLOW_CONFIG: \"/opt/airflow/config/airflow.cfg\"\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    - \"/var/run/docker.sock:/var/run/docker.sock\" # We will pass the Docker Deamon as a volume to allow the webserver containers start docker images. Ref: https://stackoverflow.com/q/51342810/7024760\n    - \"/tmp:/tmp\"\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on: &amp;airflow-common-depends-on\n    rabbitmq:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n    mongo:\n      condition: service_healthy\n\nservices:\n  postgres:\n    image: postgres:17\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n      TEST_APP_USER: test_app_user\n      TEST_APP_PASSWORD: test_app\n      TEST_APP_DB: test_app\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n      - ./init_scripts/sql/init_db.sh:/docker-entrypoint-initdb.d/init_db.sh\n      - ./init_scripts/sql:/sql\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n      interval: 10s\n      retries: 5\n      start_period: 5s\n    restart: always\n\n  mongo:\n    image: mongo:7.0.0\n    restart: always\n    healthcheck:\n      test: [\"CMD\", \"mongosh\", \"--eval\", \"db.adminCommand('ping')\"]\n      interval: 5s\n      timeout: 5s\n      retries: 3\n      start_period: 5s\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: mongo_test_db\n      MONGO_INITDB_ROOT_PASSWORD: mongo_test_db\n    ports:\n      - \"27017:27017\"\n\n  rabbitmq:\n    image: rabbitmq:4.1.0-management\n    ports:\n      - \"5672:5672\"\n      - \"15672:15672\"\n    environment:\n      - RABBITMQ_DEFAULT_USER=guest\n      - RABBITMQ_DEFAULT_PASS=guest\n    healthcheck:\n      test: rabbitmq-diagnostics -q ping\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n    volumes:\n      - rabbitmq-data:/var/lib/rabbitmq\n      - ./init_scripts/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf\n      - ./init_scripts/rabbitmq/definitions.json:/etc/rabbitmq/definitions.json\n    restart: always\n\n  airflow-apiserver:\n    &lt;&lt;: *airflow-common\n    command: api-server\n    ports:\n      - \"8080:8080\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/api/v2/version\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-scheduler:\n    &lt;&lt;: *airflow-common\n    command: scheduler\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-dag-processor:\n    &lt;&lt;: *airflow-common\n    command: dag-processor\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          'airflow jobs check --job-type DagProcessorJob --hostname \"$${HOSTNAME}\"',\n        ]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-worker:\n    &lt;&lt;: *airflow-common\n    command: celery worker\n    healthcheck:\n      # yamllint disable rule:line-length\n      test:\n        - \"CMD-SHELL\"\n        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\" || celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    environment:\n      &lt;&lt;: *airflow-common-env\n      # Required to handle warm shutdown of the celery workers properly\n      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n      DUMB_INIT_SETSID: \"0\"\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-apiserver:\n        condition: service_healthy\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-triggerer:\n    &lt;&lt;: *airflow-common\n    command: triggerer\n    healthcheck:\n      test:\n        [\n          \"CMD-SHELL\",\n          'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"',\n        ]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\n  airflow-init:\n    &lt;&lt;: *airflow-common\n    entrypoint: /bin/bash\n    # yamllint disable rule:line-length\n    command:\n      - -c\n      - |\n        if [[ -z \"${AIRFLOW_UID}\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\"\n          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"\n          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"\n          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"\n          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"\n          echo\n          export AIRFLOW_UID=$(id -u)\n        fi\n        one_meg=1048576\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\n        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)\n        disk_available=$$(df / | tail -1 | awk '{print $$4}')\n        warning_resources=\"false\"\n        if (( mem_available &lt; 4000 )) ; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\"\n          echo \"At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( cpus_available &lt; 2 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\"\n          echo \"At least 2 CPUs recommended. You have $${cpus_available}\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if (( disk_available &lt; one_meg * 10 )); then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\"\n          echo \"At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\"\n          echo\n          warning_resources=\"true\"\n        fi\n        if [[ $${warning_resources} == \"true\" ]]; then\n          echo\n          echo -e \"\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\"\n          echo \"Please follow the instructions to increase amount of resources available:\"\n          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"\n          echo\n        fi\n        echo\n        echo \"Creating missing opt dirs if missing:\"\n        echo\n        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Airflow version:\"\n        /entrypoint airflow version\n        echo\n        echo \"Files in shared volumes:\"\n        echo\n        ls -la /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Running airflow config list to create default config file if missing.\"\n        echo\n        /entrypoint airflow config list &gt;/dev/null\n        echo\n        echo \"Files in shared volumes:\"\n        echo\n        ls -la /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0\"\n        echo\n        chown -R \"${AIRFLOW_UID}:0\" /opt/airflow/\n        echo\n        echo \"Change ownership of files in shared volumes to ${AIRFLOW_UID}:0\"\n        echo\n        chown -v -R \"${AIRFLOW_UID}:0\" /opt/airflow/{logs,dags,plugins,config}\n        echo\n        echo \"Files in shared volumes:\"\n        echo\n        ls -la /opt/airflow/{logs,dags,plugins,config}\n\n    # yamllint enable rule:line-length\n    environment:\n      &lt;&lt;: *airflow-common-env\n      _AIRFLOW_DB_MIGRATE: \"true\"\n      _AIRFLOW_WWW_USER_CREATE: \"true\"\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n      _PIP_ADDITIONAL_REQUIREMENTS: \"\"\n    user: \"0:0\"\n\n  airflow-cli:\n    &lt;&lt;: *airflow-common\n    profiles:\n      - debug\n    environment:\n      &lt;&lt;: *airflow-common-env\n      CONNECTION_CHECK_MAX_COUNT: \"0\"\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\n    command:\n      - bash\n      - -c\n      - airflow\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n\n  # You can enable flower by adding \"--profile flower\" option e.g. docker-compose --profile flower up\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\n  # See: https://docs.docker.com/compose/profiles/\n  flower:\n    &lt;&lt;: *airflow-common\n    command: celery flower\n    profiles:\n      - flower\n    ports:\n      - \"5555:5555\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n      start_period: 30s\n    restart: always\n    depends_on:\n      &lt;&lt;: *airflow-common-depends-on\n      airflow-init:\n        condition: service_completed_successfully\n\nvolumes:\n  postgres-db-volume:\n  rabbitmq-data:\n</code></pre>"},{"location":"airflow/#dockerfile-running-airflow-with-additional-modulesrequirementstxt","title":"Dockerfile (Running airflow with additional modules/requirements.txt)","text":"<pre><code>FROM apache/airflow:2.7.0-python3.11\n\nUSER root\nRUN apt-get update \\\n    &amp;&amp; apt-get install -y --no-install-recommends \\\n    vim \\\n    &amp;&amp; apt-get autoremove -yqq --purge \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\nUSER airflow\n\nCOPY requirements.txt /\nRUN pip install --no-cache-dir \"apache-airflow==${AIRFLOW_VERSION}\" -r /requirements.txt\n</code></pre>"},{"location":"airflow/#populatesql-migration-on-postgres-docker-start","title":"Populate/SQL migration on Postgres docker start","text":"<pre><code>      - ./init_scripts/sql/init_db.sh:/docker-entrypoint-initdb.d/init_db.sh\n      - ./init_scripts/sql:/sql\n</code></pre> <p>Add SQL scripts to the <code>/init_scripts/sql</code> directory e.g.:</p> <pre><code>-- ./init_scripts/sql/001-init_tables.sql\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\nCREATE TABLE IF NOT EXISTS departments (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nname VARCHAR(255) NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL\n);\nCREATE TABLE IF NOT EXISTS employees (\nid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),\nemail VARCHAR(255) UNIQUE NOT NULL,\nfirst_name VARCHAR(255) NOT NULL,\nlast_name VARCHAR(255) NOT NULL,\ndepartment_id uuid NOT NULL,\ncreated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\nupdated_at TIMESTAMP WITH TIME ZONE DEFAULT NULL,\nFOREIGN KEY (department_id) REFERENCES departments(id) ON DELETE CASCADE\n);\nINSERT INTO\ndepartments(name)\nVALUES\n('HR'), ('Finance');\nINSERT INTO\nemployees(email, first_name, last_name, department_id)\nVALUES\n('mary@test_co.com', 'Mary', 'Smith', (SELECT id from departments WHERE name='Finance')),\n('dave@test_co.com', 'Dave', 'Cole', (SELECT id from departments WHERE name='Finance')),\n('jane@test_co.com', 'Jane', 'Hills', (SELECT id from departments WHERE name='Finance')),\n('john@test_co.com', 'John', 'Doe', (SELECT id from departments WHERE name='HR'));\n</code></pre> <p>Add DB migrations files or SQL commands to <code>init_db.sh</code></p> <pre><code>#!/bin/bash\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" -d \"$POSTGRES_DB\"  &lt;&lt;-EOSQL\n  CREATE USER $TEST_APP_USER WITH PASSWORD '$TEST_APP_PASSWORD';\n    CREATE DATABASE $TEST_APP_DB;\n    GRANT ALL PRIVILEGES ON DATABASE $TEST_APP_DB TO $TEST_APP_USER;\nEOSQL\npsql -U \"$TEST_APP_USER\" -d \"$TEST_APP_DB\" -a -f /sql/001-init_tables.sql\n</code></pre>"},{"location":"airflow/#rabbitmq-config-definitions","title":"RabbitMQ config &amp; definitions","text":"<p>rabbitmq.conf</p> <pre><code>load_definitions = /etc/rabbitmq/definitions.json\n</code></pre> <p>definitions.json</p> <pre><code>{\n\"rabbit_version\": \"4.1.0\",\n\"rabbitmq_version\": \"4.1.0\",\n\"product_name\": \"RabbitMQ\",\n\"product_version\": \"4.1.0\",\n\"rabbitmq_definition_format\": \"cluster\",\n\"original_cluster_name\": \"rabbit@2fa62a014fed\",\n\"explanation\": \"Definitions of cluster 'rabbit@2fa62a014fed'\",\n\"users\": [\n{\n\"name\": \"guest\",\n\"password_hash\": \"fIkEZt6PCkcBXHIEpaG4GEeW/WDN9iHQJKS23eCX6v8vrvnQ\",\n\"hashing_algorithm\": \"rabbit_password_hashing_sha256\",\n\"tags\": [\"administrator\"],\n\"limits\": {}\n}\n],\n\"vhosts\": [\n{\n\"name\": \"airflow-vhost\",\n\"description\": \"Virtual host for Airflow\",\n\"metadata\": {\n\"description\": \"Virtual host for Airflow\",\n\"tags\": [\"airflow\"],\n\"default_queue_type\": \"classic\"\n},\n\"tags\": [\"airflow\"],\n\"default_queue_type\": \"classic\"\n},\n{\n\"name\": \"rpc-vhost\",\n\"description\": \"Virtual host for RPC\",\n\"metadata\": {\n\"description\": \"Virtual host for RPC\",\n\"tags\": [\"rpc\"],\n\"default_queue_type\": \"classic\"\n},\n\"tags\": [\"rpc\"],\n\"default_queue_type\": \"classic\"\n},\n{\n\"name\": \"/\",\n\"description\": \"Default virtual host\",\n\"metadata\": {\n\"description\": \"Default virtual host\",\n\"tags\": [],\n\"default_queue_type\": \"classic\"\n},\n\"tags\": [],\n\"default_queue_type\": \"classic\"\n}\n],\n\"permissions\": [\n{\n\"user\": \"guest\",\n\"vhost\": \"airflow-vhost\",\n\"configure\": \".*\",\n\"write\": \".*\",\n\"read\": \".*\"\n},\n{\n\"user\": \"guest\",\n\"vhost\": \"rpc-vhost\",\n\"configure\": \".*\",\n\"write\": \".*\",\n\"read\": \".*\"\n},\n{\n\"user\": \"guest\",\n\"vhost\": \"/\",\n\"configure\": \".*\",\n\"write\": \".*\",\n\"read\": \".*\"\n}\n],\n\"topic_permissions\": [],\n\"parameters\": [],\n\"global_parameters\": [\n{ \"name\": \"cluster_tags\", \"value\": [] },\n{\n\"name\": \"internal_cluster_id\",\n\"value\": \"rabbitmq-cluster-id-UrGfZKlCFsC_mmTFjvTBUw\"\n}\n],\n\"policies\": [],\n\"queues\": [\n{\n\"name\": \"rpc-queue\",\n\"vhost\": \"rpc-vhost\",\n\"durable\": true,\n\"auto_delete\": false,\n\"arguments\": { \"x-queue-type\": \"classic\" }\n},\n{\n\"name\": \"stream-queue\",\n\"vhost\": \"/\",\n\"durable\": true,\n\"auto_delete\": false,\n\"arguments\": { \"x-queue-type\": \"classic\" }\n}\n],\n\"exchanges\": [],\n\"bindings\": []\n}\n</code></pre>"},{"location":"crawlers_scrapers/","title":"Crawlers scrapers","text":""},{"location":"crawlers_scrapers/#crawlerscraper-wip","title":"Crawler/Scraper (WIP)","text":""},{"location":"crawlers_scrapers/#using-scrapy-a-spike-to-crawl-a-site-and-return-list-of-pdf-files-urls-wip","title":"Using Scrapy - a spike to crawl a site and return list of PDF files URLs (WIP)","text":"<ul> <li>currently define a list if domains to ignore (mainly social media)</li> <li>exclude crawl path by patterns (e.g. file format .jpeg)</li> <li>keep track of list of webpages already crawled</li> </ul>"},{"location":"crawlers_scrapers/#need-to-be-improved","title":"Need to be improved:","text":"<ul> <li>handle cases where PDF download are hidden as javascript event e.g. click event</li> <li>implement with playwright</li> <li>handling IP blocking, rate limiting, captcha, GDPR consent popup etc</li> </ul> <pre><code>import re\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom ..items import Item\nclass CrawlerV2(CrawlSpider):\nname = \"crawler_v2\"\nstart_urls = [\"website_url_here\"]\ndeny_domains = [\n\"instagram\",\n\"facebook\",\n\"youtube\",\n\"tiktok\",\n\"linkedin\",\n\"twitter\",\n\"google\",\n\"vimeo\",\n\"disqus\",\n\"slack\",\n]\nmust_include = [\"website_domain\"]\nallowed_mime_type = [b\"application/pdf\"]\nexclude_patterns = [\".*\\.(css|js|gif|jpg|jpeg|png)\"]\nrules = [\nRule(\nLinkExtractor(\n# deny=[r\"^(tel:)\", r\"^(mailto:)\", r\"^javascript\"],\ndeny_domains=[\n\"instagram.com\",\n\"facebook.com\",\n\"youtube.com\",\n\"tiktok.com\",\n\"linkedin.com\",\n\"twitter.com\",\n\"google.com\",\n\"vimeo.com\",\n\"disqus.com\",\n\"slack.com\",\n],\n),\ncallback=\"parse_item\",\nfollow=True,\n)\n]\ndef set_playwright_true(self, request, response):\nrequest.meta[\"playwright\"] = True\nreturn request\ndef ignore_domains(self, url: str) -&gt; bool:\ncond = (\"|\").join(self.deny_domains)\nmatched = re.search(r\"({})(\\.com)\".format(cond), url, re.IGNORECASE)\nreturn bool(matched)\ndef parse_item(self, response, data=None):\ndata = data if data else {}\nif response.headers[\"Content-Type\"] in self.allowed_mime_type:\nself.logger.info({\"url\": response.url, **data})\nitem = Item()\nitem[\"inner_text\"] = data[\"inner_text\"]\nitem[\"download_attr\"] = data[\"download_attr\"]\nitem[\"title_attr\"] = data[\"title_attr\"]\nitem[\"alt\"] = data[\"alt\"]\nitem[\"src_url\"] = response.url\nyield item\nreturn\nif not any(x in response.url for x in self.must_include):\nself.logger.info(\"SKIP %s\", response.url)\nreturn\nself.logger.info(response.url)\nfor link_tag in response.css(\"a\"):\nhref = link_tag.css(\"::attr(href)\").get()\nif not href:\ncontinue\nif self.ignore_domains(href):\nself.logger.info(\"ignore_domains SKIP %s\", href)\ncontinue\nif re.search(r\"^(tel:|mailto:|fax:|javascript)\", href):\nself.logger.info(\"re.match SKIP %s\", href)\ncontinue\nreq = response.follow(\nhref,\ncallback=self.parse_item,\ncb_kwargs={\n\"data\": {\n\"inner_text\": link_tag.css(\"::text\").get(),\n\"download_attr\": link_tag.css(\"::attr(download)\").get(),\n\"title_attr\": link_tag.css(\"::attr(title)\").get(),\n\"alt\": link_tag.css(\"::attr(alt)\").get(),\n}\n},\n)\nyield req\n# async def errback(self, failure):\n#     page = failure.request.meta[\"playwright_page\"]\n#     await page.close()\n</code></pre>"},{"location":"docker/","title":"Docker","text":"<p>Docker cheat sheet</p>"},{"location":"docker/#create-and-start-containers","title":"Create and start containers","text":"<pre><code>docker-compose up\n</code></pre> <p>To run in background</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"docker/#stop-containers","title":"Stop containers","text":"<pre><code>docker-compose down\n</code></pre> <p>...and remove stopped containers</p> <pre><code>docker-compose down --remove-orphans\ndocker compose down --volumes --remove-orphans\n</code></pre>"},{"location":"docker/#docker-entrypointsh","title":"docker-entrypoint.sh","text":"<p>** For Linux ** If you want to access host from a docker container, you can find out more on how to do it here</p>"},{"location":"docker/#postgres","title":"Postgres","text":"<p>** <code>docker-compose.yml</code> example **</p> <pre><code>version: \"3.9\"\nservices:\n  postgres:\n    image: postgres:latest\n    restart: always\n    environment:\n      POSTGRES_USER: db_user\n      POSTGRES_PASSWORD: db_password\n      POSTGRES_DB: db_name\n    ports:\n      - \"5432:5432\"\n</code></pre> <p>Postgres in container to run with different port number:</p> <pre><code>ports:\n  - \"5434:5434\"\ncommand: -p 5434\n</code></pre> <p>Healthcheck on postgres container:</p> <pre><code>postgres:\n    ...\n  healthcheck:\n    test: [\"CMD-SHELL\", \"pg_isready -U db_user -d db_name -p 5432\"]\ninterval: 10s\n    timeout: 5s\n    retries: 5\n</code></pre>"},{"location":"docker/#flask","title":"Flask","text":"<p><code>Dockerfile</code></p> <pre><code>FROM python:3.13.1-slim\nRUN apt\u2212get \u2212y update\nRUN apt\u2212get install \u2212y pip3 build\u2212essential\n\nWORKDIR /app\nCOPY ./requirements.txt /app\nCOPY . ./app\nRUN pip install -r requirements.txt\n\nEXPOSE 5000\n# where the flask app initiate\nENV FLASK_APP=/app/my_app/app.py\n# or `production`, `uat` etc\nENV FLASK_ENV=development\nENV FLASK_DEBUG=1\nENV FLASK_RUN_PORT=5000\n# if not using `docker-entrypoint` mentioned earlier\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\n# or with gunicorn\n# CMD [\"gunicorn\", \"-b\", \":5000\", \"my_app.app:app\"]\n</code></pre> <p>add <code>python3 -m flask run</code> to the end of <code>docker-entrypoint.sh</code> if using <code>ENTRYPOINT</code></p> <p>Built the Docker image</p> <pre><code>docker build -t my-flask-app .\n</code></pre> <p>and run the container:</p> <pre><code>docker run -p 5000:5000 my-flask-app\n</code></pre>"},{"location":"docker/#with-docker-compose","title":"with docker compose","text":"<p>Flask + Postgres + Celery w/ Redis (Worker + Beat + Flower)</p> <pre><code>version: \"3.9\"\nservices:\n  flask_app:\n    build:\n      context: .\n    environment:\n      FLASK_ENV: development\n      FLASK_APP: /app/my_app/app.py\n      FLASK_DEBUG: 1\nFLASK_RUN_PORT: 5000\nentrypoint: ./dev-entrypoint.sh\n    volumes:\n      - .:/app\n    ports:\n      - 5000:5000\n    links:\n      - postgres\n      - redis\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n\npostgres:\n    image: postgres:latest\n    environment:\n      POSTGRES_USER: db_user\n      POSTGRES_PASSWORD: db_password\n      POSTGRES_DB: db_name\n    ports:\n      - \"5432:5432\"\nhealthcheck:\n      test:[\"CMD-SHELL\", \"pg_isready -U db_user -d db_name\"]\ninterval: 5s\n      timeout: 5s\n      retries: 5\nredis:\n    image: redis:latest\n    ports:\n      - \"6379:6379\"\nhealthcheck:\n      test: [\"CMD\", \"redis-cli\", \"--raw\", \"incr\", \"ping\"]\ncelery-worker:\n    build:\n      context: .\n    hostname: worker\n    entrypoint: celery\n    command: -A my_app.celery worker --loglevel=info  # change `my_app.celery` to where celery is init\n    volumes:\n      - .:/app\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n\ncelery-beat:\n    build:\n      context: .\n    hostname: beat\n    entrypoint: celery\n    command: -A my_app.celery beat --loglevel=info\n    volumes:\n      - .:/app\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n\ncelery-flower:\n    build:\n      context: .\n    hostname: flower\n    entrypoint: celery\n    command: -A my_app.celery flower --loglevel=info\n    volumes:\n      - .:/app\n        ports:\n      - 5555:5555\n    links:\n      - redis\n    depends_on:\n      - flask_app\n      - redis\n</code></pre>"},{"location":"git/","title":"Git","text":""},{"location":"git/#multi-git-accounts-access","title":"Multi git accounts access","text":"<p>If you work on different projects for different companies, or just want to separate company work from your own personal projects. You can use <code>includeIf</code> to tell git which identity/account to use depending on the directory path - for example, projects under <code>~/work/</code> folder are linked to the work account, <code>~/personal/</code> ones are linked to personal account</p> <p>First, in <code>~/.gitconfig</code> we do</p> <pre><code>[includeIf \"gitdir:~/work/\"]\n    path = .gitconfig-work\n[includeIf \"gitdir:~/personal/\"]\n    path = .gitconfig-personal\n</code></pre> <p>then edit/create (if haven't got one) <code>~/.gitconfig-personal</code> and add the git config of your personal account</p> <pre><code>[user]\nname = personalAccName\nemail = personal-acc@email-address-here.com\n</code></pre> <p>do the same for <code>~/.gitconfig-work</code> but with your work account info</p> <pre><code>[user]\nname = workAccName\nemail = work-acc@email-address-here.com\n</code></pre>"},{"location":"git/#add-remote-repo","title":"Add remote repo","text":"<pre><code>git remote add origin https://github.com/OWNER/REPOSITORY.git\n</code></pre>"},{"location":"git/#list-existing-remotes","title":"List existing remotes","text":"<pre><code>git remote -v\n</code></pre>"},{"location":"git/#change-remote-repos-url","title":"Change remote repo's URL","text":"<pre><code>git remote set-url origin https://github.com/OWNER/REPOSITORY.git\n</code></pre>"},{"location":"git/#rebse-branch-to-another","title":"Rebse branch to another","text":"<pre><code>git rebase origin/branch_name\n</code></pre>"},{"location":"git/#squash-amend-reword-commits-with-interactive-rebase-git-rebase-i","title":"Squash, amend, reword commits with interactive rebase (<code>git rebase -i</code>)","text":""},{"location":"git/#reword-commit-message","title":"Reword commit message","text":"<p>For example, I want to reword one of the last 3 commits</p> <pre><code>git rebase -i HEAD~3\n</code></pre> <p>will output the following</p> <pre><code>pick c2a4d95 reword me please\npick 2a4e409 update SQL query to insert new employee records\npick 08c8836 Refactor and clean up employee class\n\n# Rebase 8d8a4b3..08c8836 onto 8d8a4b3 (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n</code></pre> <p>To reword commit message <code>c2a4d95</code>, we replace <code>pick</code> with <code>reword</code>/<code>r</code> for that commit, then save and exit</p> <pre><code>reword c2a4d95 reword me please\npick 2a4e409 update SQL query to insert new employee records\npick 08c8836 Refactor and clean up employee class\n</code></pre> <p>we can now amend the commit message on the next \"screen\" - update the message then save and exit again:</p> <pre><code>Update documentation\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n...\n</code></pre> <p>It should output a message similar to the one below if rebased successfully:</p> <pre><code>[detached HEAD 7a8daf8] Update documentation\n Date: Sat Apr 22 10:01:48 2023 +0100\n 1 file changed, 71 insertions(+)\n create mode 100644 docs/docker.md\nSuccessfully rebased and updated refs/heads/rebase-test.\n</code></pre>"},{"location":"git/#squash-commits","title":"Squash commits","text":"<p>Using the same example from <code>reword</code> above, we now want to squash the three commits into one and amend the first commit message again:</p> <pre><code>reword 7a8daf8 Update documentation\nfixup 96dce5b update SQL query to insert new employee records\nfixup 467b4cc Refactor and clean up employee class\n</code></pre> <p>Here we use <code>fixup</code>/<code>f</code> instead of <code>squash</code>/<code>s</code> as we are discarding all the other commit messages except the top one.</p> <p>Again we should get <code>Successfully rebased and updated ...</code> if rebased successfully.</p>"},{"location":"localstack/","title":"Localstack","text":"<p>LocalStack provides a fully functional AWS cloud stack that can be run locally for development and testing purposes without using the actual AWS cloud services.</p>"},{"location":"localstack/#install-localstack-and-run","title":"Install Localstack and run","text":"<pre><code>pip install localstack\nlocalstack start -d\n</code></pre> <p>To get the status of each service</p> <pre><code>localstack status services\n</code></pre>"},{"location":"localstack/#run-via-docker-compose","title":"Run via Docker compose","text":"<p>** <code>docker-compose.yml</code> for Localstack S3, SQS and DynamoDB **</p> <pre><code>version: \"3.9\"\nservices:\n  localstack:\n    container_name: \"${LOCALSTACK_DOCKER_NAME-localstack_main}\"\nimage: localstack/localstack:latest\n    ports:\n      - \"4566:4566\"\n- \"4510-4559:4510-4559\"\nenvironment:\n      - DEBUG=1\n- DOCKER_HOST=unix:///var/run/docker.sock\n            - SERVICES=s3,sqs,dynamodb\n    volumes:\n      - \"${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack\"\n- \"/var/run/docker.sock:/var/run/docker.sock\"\n- \"./data:/tmp/localstack\"\nhealthcheck:\n            test:\n        - CMD\n        - bash\n        - -c\n        - awslocal dynamodb list-tables\n          &amp;&amp; awslocal s3 ls\n          &amp;&amp; awslocal sqs list-queues\n            interval: 10s\n            timeout: 5s\n            retries: 5\n</code></pre>"},{"location":"localstack/#populating-data-into-localstack-s3-on-docker-compose-up","title":"Populating data into localstack S3 on <code>docker compose up</code>","text":"<p>Add these two lines to the <code>volumes</code> in the <code>docker-compose.yml</code> (assuming the data/files we want to upload to loaclstack S3 are in <code>s3_data</code> folder)</p> <pre><code>      - \"./.localstack:/etc/localstack/init/ready.d\"\n      - \"./s3_data:/s3_data\" # location of files used to pre-populate the Localstack S3 bucket\n</code></pre> <p>Then create <code>.localstack</code> directory, and inside this folder create a shell script (e.g. <code>create_and_populate_bucket.sh</code>) with commands to create and upload/synchronise the localstack S3 bucket with local data:</p> <pre><code>awslocal s3api create-bucket --bucket mock-bucket\nawslocal s3api put-bucket-policy --bucket mock-bucket --policy \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Sid\\\":\\\"PublicReadGetObject\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\\\"*\\\",\\\"Action\\\":\\\"s3:GetObject\\\",\\\"Resource\\\":\\\"arn:aws:s3:::mock-bucket/*\\\"}]}\"\nawslocal s3 sync /s3_data s3://mock-bucket\n</code></pre> <p>The bucket policy is basically the inline version of this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::mock-bucket/*\"\n        },\n    ]\n}\n</code></pre>"},{"location":"Javascript/datatable/","title":"Datatable","text":""},{"location":"Javascript/datatable/#reload-tables-with-new-data","title":"Reload table(s) with new data","text":"<pre><code>const reloadDatableData = () =&gt; {\nconst dataUrl = `/data-api-endpoint`;\nfetch(dataUrl)\n.then((response) =&gt; response.json())\n.then((data) =&gt; {\nconst datatable = $(\"#datatable-id\").DataTable();\ndatatable.clear().rows.add(data).draw();\n// in case of table width not working properly (responsive)\nsetTimeout(function () {\n$(\"#datatable-id\").DataTable().columns.adjust().draw();\n}, 100);\n});\n};\n</code></pre>"},{"location":"Javascript/datatable/#rezise-tables-to-fit-container-in-bootstrap-v5-tabs","title":"Rezise table(s) to fit container in bootstrap (v5) tabs","text":"<pre><code>const initTabsEvts = () =&gt; {\nconst tabEl = document.querySelectorAll('a[data-bs-toggle=\"tab\"]');\ntabEl.forEach((ele) =&gt; {\nele.addEventListener(\"shown.bs.tab\", function (event) {\nconst datatables = document.querySelector(\".tab-pane.active .datatable\");\ndatatables.forEach((datatable) =&gt; {\nsetTimeout(function () {\n$(datatable).DataTable().columns.adjust().draw();\n}, 50);\n});\n});\n});\n};\n</code></pre>"},{"location":"Javascript/html-to-image/","title":"Html to image","text":""},{"location":"Javascript/html-to-image/#generate-an-image-from-a-dom-node-and-exportdownload","title":"Generate an image from a DOM node (and export/download)","text":"<p>Follow html-to-image GitHub repo for install instruction or get min.js from cdnjs</p> <p>E.g. trying to generate and download an image of <code>div#screenshot-me</code>:</p> <pre><code>&lt;div id=\"screenshot-me\"&gt;\n&lt;div&gt;\n&lt;img src=\"https://picsum.photos/id/29/536/354\" /&gt;\n&lt;/div&gt;\n&lt;p&gt;\n    Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo\n    ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis\n    parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec,\n    pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec\n    pede justo, fringilla vel, aliquet nec, vulputate eget, arcu.\n  &lt;/p&gt;\n&lt;p&gt;\n    In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam\n    dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus.\n    Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo\n    ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem\n    ante, dapibus in, viverra quis, feugiat a, tellus.\n  &lt;/p&gt;\n&lt;p&gt;\n    Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean\n    imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies\n    nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum\n    rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum.\n    Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem.\n  &lt;/p&gt;\n&lt;/div&gt;\n&lt;button\nid=\"screenshot-btn\"\nonclick=\"takeCardScreenShot('screenshot-me', 'screenshot.png')\"\n&gt;\n  Take a screenshot!\n&lt;/button&gt;\n</code></pre> <p>JavaSrcipt to generate the image and trigger download (html-to-image v 1.11.11)</p> <pre><code>const takeScreenShot = (eleId, filename) =&gt; {\nhtmlToImage\n.toCanvas(document.getElementById(eleId), {\nquality: 1,\nbackgroundColor: \"#FFFFFF\",\n})\n.then((canvas) =&gt; {\nimgSaveAs(canvas.toDataURL(\"image/png\"), `${filename}.png`);\n})\n.catch((error) =&gt; {\nconsole.log(error);\n});\n};\nconst imgSaveAs = (uri, filename) =&gt; {\nconst link = document.createElement(\"a\");\nif (typeof link.download === \"string\") {\nlink.href = uri;\nlink.download = filename;\ndocument.body.appendChild(link);\nlink.click();\ndocument.body.removeChild(link);\n} else {\nwindow.open(uri);\n}\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/","title":"Javascript notes","text":""},{"location":"Javascript/javascript%20notes/#copy-to-clipboard","title":"Copy to clipboard","text":"<pre><code>const copyToClipboard = (txt) =&gt; {\ntry {\nnavigator.clipboard.writeText(url);\n} catch (e) {\nunsecuredCopyToClipboard(txt);\n}\nshowToastNotification(\"Copied to clipboard\", \"success\");\n};\nconst unsecuredCopyToClipboard = (text) =&gt; {\nconst ta = document.createElement(\"textarea\");\nta.value = text;\ndocument.body.appendChild(ta);\nta.focus();\nta.select();\ntry {\ndocument.execCommand(\"copy\");\n} catch (err) {\nconsole.error(\"Unable to copy to clipboard\", err);\n}\ndocument.body.removeChild(ta);\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#fetch","title":"fetch","text":"<pre><code>const fetchExample = (url) =&gt; {\nfetch(url, {\nmethod: \"POST\",\nheaders: {\nAccept: \"application/json\",\n\"Content-Type\": \"application/json\",\n},\nbody: JSON.stringify({ ... }),\n})\n.then((res) =&gt; {\nif (res.ok) {\nreturn res.json();\n}\nreturn Promise.reject(res);\n})\n.then((body) =&gt; {\n...\n})\n.catch((error) =&gt; {\n...\n})\n.finally(() =&gt; {\n...\n});\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#download-blobfile-with-fetch","title":"Download blob/file with fetch","text":"<pre><code>const downloadFile = (fileUrl) =&gt; {\nfetch(fileUrl, {\nheaders: {\nAccept: \"application/json\",\n\"Content-Type\": \"application/json\",\n},\n})\n.then((response) =&gt; {\nif (!response.ok) {\nreturn response.json().then((body) =&gt; {\n// error handling\nif (body.status == \"error\") {\nreturn downloadErrorHandler();\n}\nthrow new Error(\"HTTP status \" + response.status);\n});\n}\nreturn response.blob().then((blob) =&gt; {\nfilename = extractFilenameFromHeaders(response.headers);\nreturn downloadBlob(blob, filename);\n});\n})\n.catch((error) =&gt; {\nconsole.log(error);\n});\n};\nconst downloadBlob = (blob, filename) =&gt; {\nconst link = document.createElement(\"a\");\nlink.href = window.URL.createObjectURL(blob);\nlink.download = filename;\nlink.click();\nlink.remove();\n};\n// Extract filename from headers\nconst extractFilenameFromHeaders = (headers) =&gt; {\nconst header = headers.get(\"Content-Disposition\");\nconst parts = header.split(\";\");\nreturn parts[1].split(\"=\")[1];\n};\n</code></pre>"},{"location":"Javascript/javascript%20notes/#open-url-on-same-tab","title":"Open url on same tab","text":"<pre><code>window.location.href = url;\n</code></pre>"},{"location":"Javascript/javascript%20notes/#open-url-on-new-tab","title":"Open url on new tab","text":"<pre><code>window.open(url, \"_blank\");\n</code></pre>"},{"location":"Javascript/javascript%20notes/#axios-link","title":"Axios Link","text":"<pre><code>axios\n.get(\"/url\", {\nheaders: { \"Content-Type\": \"application/json\" },\n})\n.then((response) =&gt; {\nloadData(response.data);\n...\n})\n.catch(function (error) {\nconsole.error(error);\n});\n</code></pre>"},{"location":"Javascript/javascript%20notes/#abort-controller","title":"Abort Controller","text":"<pre><code>let abortController = new AbortController();\nconst abortRequestAndResetController = (controller) =&gt; {\nif (controller) {\ncontroller.abort();\n}\nreturn new AbortController();\n};\n// For Bootstrap 5 modal on open/close\nconst modal = document.getElementById(\"modal_id\");\nmodal.addEventListener(\"hidden.bs.modal\", () =&gt; {\nabortController.abort();\n});\nmodal.addEventListener(\"show.bs.modal\", () =&gt; {\nabortController = new AbortController();\n});\n// Fetch\nfetch(\"/url\", {\nmethod: \"GET\",\nheaders: {\nAccept: \"application/json\",\n\"Content-Type\": \"application/json\",\n},\nbody: JSON.stringify(params),\nsignal: abortController.signal,\n})\n.then((res) =&gt; res.json())\n.then((data) =&gt; {\n...\n})\n.catch((error) =&gt; {\nif (error.name === \"AbortError\" || error.name === \"CanceledError\") {\nconsole.log(\"Fetch aborted\");\n} else {\nconsole.log(error);\n}\n});\n// Axios\naxios\n.get(\"/url\", {\nheaders: { \"Content-Type\": \"application/json\" },\n})\n.then((response) =&gt; {\nloadData(response.data);\n...\n})\n.catch(function (error) {\nconsole.error(error);\n});\n</code></pre>"},{"location":"Javascript/javascript%20notes/#simple-table-class-and-manager","title":"Simple table class and manager","text":"<pre><code>class SimpleTable {\nconstructor(containerId, tableId, columnsConfig, rows, cssClasses) {\nthis.containerId = containerId;\nthis.tableId = tableId;\nthis.columnsConfig = columnsConfig;\nthis.rows = rows;\nthis.cssClasses = cssClasses;\n}\ngetData = () =&gt; {\nreturn this.rows;\n};\ngetTextAlign = (hozAlign) =&gt; {\nif (!hozAlign) {\nreturn \"\";\n}\nswitch (hozAlign) {\ncase \"right\":\nreturn \"text-end\";\ncase \"left\":\nreturn \"text-start\";\ndefault:\n`text-${hozAlign}`;\n}\n};\ngenerateTHead = () =&gt; {\nconst thRows = this.generateThRows(this.columnsConfig);\nreturn `&lt;thead&gt;\n${thRows.join(\"\")}\n    &lt;/thead&gt;`;\n};\ngenerateThRows = (columnsConfig, currentLevel = 0) =&gt; {\nconst rows = [];\nconst currentRow = [];\ncolumnsConfig.forEach((columnConfig) =&gt; {\nconst textAlign = this.getTextAlign(columnConfig.hozAlign);\nconst fieldName = columnConfig.field ? columnConfig.field : \"\";\nconst rowSpan = columnConfig.headerRowSpan\n? `rowspan=\"${columnConfig.headerRowSpan}\"`\n: \"\";\nif (columnConfig.columns) {\ncurrentRow.push(`&lt;th class=\"${fieldName} ${textAlign}\" ${rowSpan} colspan=\"${columnConfig.columns.length}\"&gt;\n${columnConfig.title}&lt;/th&gt;`);\nconst nestedThRows = this.generateThRows(\ncolumnConfig.columns,\ncurrentLevel + 1\n);\nrows.push(...nestedThRows);\n} else {\ncurrentRow.push(\n`&lt;th class=\"${fieldName} ${textAlign}\" ${rowSpan}&gt;${columnConfig.title}&lt;/th&gt;`\n);\n}\n});\nrows.unshift(`&lt;tr&gt;${currentRow.join(\"\")}&lt;/tr&gt;`);\nreturn rows;\n};\ngenerateTr = (rowData) =&gt; {\nconst tds = this.columnsConfig\n.map((columnConfig) =&gt; {\nif (columnConfig.columns) {\nreturn columnConfig.columns\n.map((nestedColumnConfig) =&gt; {\nreturn this.generateTd(\nrowData[nestedColumnConfig.field],\nnestedColumnConfig\n);\n})\n.join(\"\");\n} else {\nreturn this.generateTd(rowData[columnConfig.field], columnConfig);\n}\n})\n.join(\"\");\nreturn `&lt;tr&gt;${tds}&lt;/tr&gt;`;\n};\ngenerateTBody = () =&gt; {\nif (!this.rows?.length) {\nconst colspan = this.columnsConfig.length;\nreturn `&lt;tbody&gt;\n        &lt;tr&gt;&lt;td colspan=\"${colspan}\"&gt;No data available&lt;/td&gt;&lt;/tr&gt;\n      &lt;/tbody&gt;`;\n}\nconst trs = this.rows\n.map((rowData) =&gt; {\nreturn this.generateTr(rowData);\n})\n.join(\"\");\nreturn `&lt;tbody&gt;${trs}&lt;/tbody&gt;`;\n};\ngenerateTd = (cellData, columnConfig) =&gt; {\nconst textAlign = this.getTextAlign(columnConfig.hozAlign);\nconst styles = columnConfig.styles ? columnConfig.styles.join(\" \") : \"\";\nconst value = valFormatter(\ncellData,\ncolumnConfig.formatter,\ncolumnConfig.formatterParams\n);\nreturn `&lt;td class=\"${columnConfig.field} ${textAlign} ${styles}\"&gt;${value}&lt;/td&gt;`;\n};\nrender = () =&gt; {\nconst container = document.getElementById(this.containerId);\nif (container) {\nconst tableClasses = this.cssClasses.tableStyle\n? this.cssClasses.tableStyle\n: \"\";\ncontainer.innerHTML = `\n      &lt;table id=\"${this.tableId}\" class=\"table ${tableClasses}\"&gt;\n${this.generateTHead()}\n${this.generateTBody()}\n      &lt;/table&gt;\n      `;\n}\n};\nrefreshAndRender = (data) =&gt; {\nthis.rows = data;\nthis.render();\n};\n}\nclass SimpleTableManager {\nconstructor() {\nthis.tables = {};\n}\ncreate = (containerId, tableId, columns, rows, cssClasses) =&gt; {\nif (this.tables[tableId]) {\nconsole.error(`Table ${tableId} already exists`);\nreturn;\n}\nconst table = new SimpleTable(\ncontainerId,\ntableId,\ncolumns,\nrows,\ncssClasses\n);\nthis.tables[tableId] = table;\nreturn table;\n};\nfind = (tableId) =&gt; {\nconst table = this.tables[tableId];\nif (!table) {\nreturn null;\n}\nreturn table;\n};\nfind_or_create = (containerId, tableId, columns, rows, cssClasses) =&gt; {\nconst table = this.find(tableId);\nif (table) {\nreturn table;\n}\nreturn this.create(containerId, tableId, columns, rows, cssClasses);\n};\nrenderTable = (tableId) =&gt; {\nconst table = this.find(tableId);\nif (table) {\ntable.render();\n}\n};\nrefreshTableData = (tableId, data) =&gt; {\nconst table = this.find(tableId);\nif (table) {\ntable.refreshAndRender(data);\n}\n};\n}\nexport { SimpleTableManager, SimpleTable };\n</code></pre>"},{"location":"Javascript/plotly/","title":"Plotly","text":""},{"location":"Javascript/plotly/#plotly-js","title":"Plotly JS","text":"<p>Placeholder for chart in HTML:</p> <pre><code>&lt;div id=\"chart-id-here\" class=\"plotly-charts\"&gt;&lt;/div&gt;\ninitChart(\"chart-id-here\");\n</code></pre> <p>Javascript to fetch chart data from API endpoint</p> <pre><code>const initChart = (eleId) =&gt; {\nfetch(\"/api-endpoint-for-chart-data\")\n.then((response) =&gt; response.json())\n.then((data) =&gt; {\nconst chartData = data.chart_data;\nconst plotConfigData = data.config_data;\nplotlyChart(eleId, chartData, null, plotConfigData);\n});\n};\n</code></pre> <pre><code>const plotlyChartLayout = (title, opts) =&gt; {\nlet layout = $.extend(\n{\ntitle: {\ntext: title,\nfont: {\nsize: 12,\n},\n},\nxaxis: {\ntickfont: {\nsize: 11,\n},\n},\nyaxis: {\nautomargin: true,\ntickfont: {\nsize: 11,\n},\n},\nshowlegend: true,\nlegend: {\nfont: {\nsize: 10,\n},\n},\nbargap: 0.2,\nfont: { color: \"#4e4e4e\" },\n},\nopts\n);\nreturn layout;\n};\nconst plotlyChart = (\ntargetEle,\ndata,\ntitle,\nopts = null,\nconfigOpts = null,\nrefreshData = false\n) =&gt; {\nconst layout = plotlyChartLayout(title, opts);\nconst imgFilename = opts.filename ? opts.filename : \"plot-image\";\nconst config = $.extend(\n{\nresponsive: true,\ndisplayModeBar: true,\n// to hide the buttons/chart functions we don't want\nmodeBarButtonsToRemove: [\n\"zoom2d\",\n\"pan2d\",\n\"select2d\",\n\"lasso2d\",\n\"zoomIn2d\",\n\"zoomOut2d\",\n\"autoScale2d\",\n\"resetScale2d\",\n],\ntoImageButtonOptions: {\nformat: \"png\",\nfilename: imgFilename,\nscale: 2,\n},\ndisplaylogo: false,\n},\nconfigOpts\n);\nif (refreshData) {\nPlotly.react(targetEle, data, layout, config);\n} else {\nPlotly.newPlot(targetEle, data, layout, config);\n}\n};\n</code></pre>"},{"location":"Javascript/plotly/#rezise-charts-to-fit-container-in-bootstrap-v5-tabs","title":"Rezise chart(s) to fit container in bootstrap (v5) tabs","text":"<pre><code>const initTabsEvts = () =&gt; {\nconst tabEl = document.querySelectorAll('a[data-bs-toggle=\"tab\"]');\ntabEl.forEach((ele) =&gt; {\nele.addEventListener(\"shown.bs.tab\", function (event) {\nconst charts = document.querySelectorAll(\n\".tab-pane.active .plotly-charts\"\n);\ncharts.forEach((chart) =&gt; {\nPlotly.relayout(chart, { autosize: true });\n});\n});\n});\n};\n</code></pre>"},{"location":"Javascript/tabulator/","title":"Tabulator","text":"<p>Tabulator</p> <pre><code>const initTable = (dataUrl) =&gt; {\nconst table = new Tabulator(\"#table\", {\najaxURL: dataUrl,\nlayout: \"fitColumns\",\nindex: \"id\",\nmaxHeight: \"50vh\",\nresizableColumns: false,\nplaceholder: \"No Data Available\",\ninitialSort: [{ column: \"timestamp\", dir: \"desc\" }],\ncolumns: [\n{\nfield: \"id\",\ntitle: \"ID\",\nsorter: \"string\",\nvisible: false,\n},\n{\nfield: \"timestamp\",\ntitle: \"Timestamp\",\nsorter: \"datetime\",\nformatter: \"datetime\",\nformatterParams: {\ninputFormat: \"yyyy-MM-dd HH:mm:ss\",\noutputFormat: \"yyyy-MM-dd HH:mm:ss\",\n},\nheaderFilter: false,\n},\n{\nfield: \"date_val\",\ntitle: \"Date value\",\nsorter: \"date\",\nsorterParams: {\nalignEmptyValues: \"bottom\",\nformat: \"yyyy-MM-dd\",\n},\nformatter: \"date\",\nformatterParams: {\ninputFormat: \"yyyy-MM-dd\",\noutputFormat: \"yyyy-MM-dd\",\n},\nheaderFilter: true,\n},\n{\nfield: \"boolean_val\",\ntitle: \"Boolean value\",\nsorter: \"boolean\",\nformatter: \"tickCross\",\nheaderFilter: \"tickCross\",\nheaderFilterParams: { tristate: true },\nhozAlign: \"center\",\nformatterParams: {\nallowEmpty: true,\nallowTruthy: true,\ntickElement: \"Yes\",\ncrossElement: false,\n},\n},\n{\nfield: \"icon\",\ntitle: \"Icon\",\nheaderFilter: false,\nhozAlign: \"center\",\nformatter: iconLink,\n},\n],\n});\n};\nconst iconLink = (cell, formatterParams) =&gt; {\nconst val = cell.getValue();\nif (!!val) {\nreturn `&lt;i class=\"fe fe-link pointer\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" title=\"${val}\"&gt;&lt;/i&gt;`;\n}\nreturn \"\";\n};\nconst refreshTableDate = (dataUrl) =&gt; {\ntable.replaceData(dataUrl);\n};\n</code></pre>"},{"location":"Python/pandas/","title":"Pandas","text":""},{"location":"Python/pandas/#flatten-entityattributevalue-modelstables-using-pandas-dataframe","title":"Flatten entity\u2013attribute\u2013value models/tables using Pandas dataframe","text":"<p>users (entity/object table)</p> id name email 1 John Smith john.smith@example.com 2 Jane Smith jane.smith@example.com <p>page_config (attribute table)</p> id name 1 font-size 2 color 3 background-color <p>users_page_config (EAV table)</p> id user_id page_config_id value 1 1 1 12px 2 1 2 #3e3e3e 3 1 3 blue 4 2 1 16px 5 2 2 #000000 6 2 3 green <pre><code>import numpy as np\nimport pandas as pd\nclass EavFlattener:\ndef flatten_eav_table(self, data):\nresult_df = pd.DataFrame(data)\nresult_df = result_df.pivot(index=\"user_id\", columns='attribute')['attribute_value']\nresult_df = result_df.reset_index()\nresult_df = result_df.drop(np.nan, axis=1)\nreturn result_df\ndata = [\n{'user_id': 1, 'attribute': 'font-size', 'attribute_value': '12px'},\n{'user_id': 1, 'attribute': 'color', 'attribute_value': '#3e3e3e'},\n{'user_id': 1, 'attribute': 'background-color', 'attribute_value': 'blue'},\n{'user_id': 2, 'attribute': 'font-size', 'attribute_value': '16px'},\n{'user_id': 2, 'attribute': 'color', 'attribute_value': '#000000'},\n{'user_id': 2, 'attribute': 'background-color', 'attribute_value': 'green'}\n]\nEavFlattener().flatten_eav_table(data)\n# Flattened EAV data\n# [{'user_id': 1, 'background-color': 'blue', 'color': '#3e3e3e', 'font-size': '12px'},\n#  {'user_id': 2, 'background-color': 'green', 'color': '#000000', 'font-size': '16px'}]\n</code></pre>"},{"location":"Python/pyJWT/","title":"pyJWT","text":"<p>PyJWT</p>"},{"location":"Python/pyJWT/#validate-and-return-user-with-aws-cognito-access-token","title":"Validate and return user with AWS cognito access token","text":"<pre><code>import json\nfrom typing import Optional\nimport jwt\nimport requests\nfrom jwt.algorithms import RSAAlgorithm\nfrom path.to.user_model import User\nfrom path.to.config import config\nclass UserAuthService:\ndef get_current_user(self, access_token: str = None) -&gt; Optional[User]:\nvalid_token, current_user = self.validate_and_return_credentials(access_token)\nreturn current_user if (valid_token and current_user) else None\ndef get_public_access_keys(self) -&gt; str:\nauth = config.auth\nurl = f\"https://cognito-idp.{auth.region}.amazonaws.com/{auth.identity_pool_id}/.well-known/jwks.json\"\nresponse = requests.get(url)\nreturn response.text\ndef find_public_key(self, kid: str) -&gt; tuple[bool, Optional[dict]]:\npublic_keys = self.get_public_access_keys()\npublic_keys = json.loads(public_keys)\npublic_keys = {key[\"kid\"]: key for key in public_keys[\"keys\"]}\nmatched_key = public_keys.get(kid)\nreturn bool(matched_key), matched_key\ndef decode_access_token(self, public: dict, access_token: str, alg: str) -&gt; tuple[bool, dict]:\ntry:\npublic_key = RSAAlgorithm.from_jwk(json.dumps(public))\npayload = jwt.decode(\naccess_token,\npublic_key,\nalgorithms=[alg],\nverify=True,\noptions={\n\"verify_exp\": True,\n\"verify_nbf\": True,\n\"verify_iat\": False, # True\n\"verify_aud\": False, # True\n\"verify_iss\": True,\n},\naudience=config.auth.identity_pool_web_client,\nissuer=f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\",\n)\nexcept jwt.exceptions.DecodeError as exc:\nreturn False, {}\n# This should be check if token_use is ID token and \"verify_aud\" in jwt decode\nif payload[\"token_use\"] != \"access\":\nreturn False, {}\nif payload[\"client_id\"] != config.auth.identity_pool_web_client:\nreturn False, {}\nreturn True, payload\ndef validate_and_return_credentials(self, access_token: str) -&gt; tuple[bool, Optional[User]]:\nvalid_token, current_user = False, None\ntry:\nheaders = jwt.get_unverified_header(access_token)\nvalid_pub_key, user_public_key = self.find_public_key(headers.get(\"kid\"))\nif not user_public_key:\nreturn False, None\nvalid_signature, payload = self.decode_access_token(user_public_key, access_token, headers.get(\"alg\"))\nif valid_pub_key and valid_signature:\nvalid_token = True\nuser_sub_id = payload.get(\"sub\")\ncurrent_user = User.find_by(guid=user_sub_id) if user_sub_id else None\nexcept Exception as exc:\nreturn False, None\nreturn valid_token, current_user\n</code></pre>"},{"location":"Python/pyJWT/#with-alb-token-verification-alb-beast-vulnerability","title":"With ALB token verification (ALB Beast vulnerability)","text":"<pre><code>import json\nfrom typing import Optional, Tuple\nimport jwt\nimport requests\nfrom jwt.algorithms import RSAAlgorithm\nfrom app.config import config\nfrom path.to.user_model import User\nclass UserAuthService:\ndef get_current_user(self, access_token: str = None, data_token: str = None):\nis_valid_alb_token = self.verify_alb_token(data_token)\nif not is_valid_alb_token:\nreturn None\nvalid_token, current_user = self.validate_access_token_and_return_credentials(\naccess_token\n)\nreturn current_user if (valid_token and current_user) else None\ndef get_public_access_keys(self, url: str) -&gt; Optional[str]:\ntry:\nresponse = requests.get(url)\nresponse.raise_for_status()\nreturn response.text\nexcept Exception as exc:\nreturn None\ndef find_public_key(self, kid: str) -&gt; Tuple[bool, Optional[dict]]:\nauth = config.auth\nurl = f\"https://cognito-idp.{auth.region}.amazonaws.com/{auth.identity_pool_id}/.well-known/jwks.json\"\npublic_keys = self.get_public_access_keys(url)\nif not public_keys:\nreturn False, {}\npublic_keys = json.loads(public_keys)\npublic_keys = {key[\"kid\"]: key for key in public_keys[\"keys\"]}\nmatched_key = public_keys.get(kid)\nreturn bool(matched_key), matched_key\ndef decode_access_token(self, public: dict, access_token: str, alg: str) -&gt; tuple[bool, dict]:\ntry:\npublic_key = RSAAlgorithm.from_jwk(json.dumps(public))\npayload = jwt.decode(\naccess_token,\npublic_key,\nalgorithms=[alg],\nverify=True,\noptions={\n\"verify_exp\": True,\n\"verify_nbf\": True,\n\"verify_iat\": False,\n\"verify_aud\": False,\n\"verify_iss\": True,\n},\naudience=config.auth.identity_pool_web_client,\nissuer=f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\",\n)\nexcept jwt.exceptions.DecodeError as exc:\nreturn False, {}\n# This should be check if token_use is ID token and \"verify_aud\" in jwt decode\nif payload[\"token_use\"] != \"access\":\nreturn False, {}\nif payload[\"client_id\"] != config.auth.identity_pool_web_client:\nreturn False, {}\nreturn True, payload\ndef verify_alb_token(self, data_token: str) -&gt; bool:\nif not data_token:\nreturn False\nheaders = jwt.get_unverified_header(data_token)\nauth = config.auth\nexpected_signer = (\nf\"arn:aws:elasticloadbalancing:{auth.region}:{auth.alb_id}:\"\nf\"loadbalancer/app/{auth.load_balancer_name}/{auth.load_balancer_id}\"\n)\nif not headers.get(\"signer\") == expected_signer:\nreturn False\nurl = f'https://public-keys.auth.elb.{auth.region}.amazonaws.com/{headers.get(\"kid\")}'\npublic_key = self.get_public_access_keys(url)\nif not public_key:\nreturn False\nexpected_issuer = f\"https://cognito-idp.{config.auth.region}.amazonaws.com/{config.auth.identity_pool_id}\"\npayload = jwt.decode(\ndata_token,\npublic_key,\nalgorithms=[headers[\"alg\"]],\nverify=True,\noptions={\n\"verify_exp\": True,\n\"verify_nbf\": True,\n\"verify_iat\": False,\n\"verify_aud\": False,\n\"verify_iss\": True,\n},\naudience=config.auth.identity_pool_web_client,\nissuer=expected_issuer,\n)\nreturn payload.get(\"iss\") == expected_issuer\ndef validate_access_token_and_return_credentials(self, access_token: str) -&gt; Tuple[bool, Optional[str], bool]:\nvalid_token, user_sub_id, current_user = False, None, None\ntry:\nheaders = jwt.get_unverified_header(access_token)\nvalid_pub_key, user_public_key = self.find_public_key(headers.get(\"kid\"))\nif not user_public_key:\nreturn False, None, None\nvalid_signature, payload = self.decode_access_token(user_public_key, access_token, headers.get(\"alg\"))\nif valid_pub_key and valid_signature:\nvalid_token = True\nuser_sub_id = payload.get(\"sub\")\ncurrent_user = User.find_by(guid=user_sub_id) if user_sub_id else None\nexcept Exception as exc:\npass\nreturn valid_token, current_user\n</code></pre>"},{"location":"Python/python%20notes/","title":"Python notes","text":""},{"location":"Python/python%20notes/#logging-class","title":"Logging class","text":"<pre><code>import logging\nclass Logger:\ndef __init__(self):\nself._logger = None\ndef setup_logger(self):\nlogger = logging.getLogger(\"root\")\nif logger.handlers:\nreturn logger\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(module)s %(funcName)s - %(message)s\")\nhandler = logging.StreamHandler()\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nreturn logger\ndef get_logger(self):\nif self._logger is None:\nself._logger = self.setup_logger()\nreturn self._logger\n</code></pre> <p>To use:</p> <pre><code>from path.to.logger import Logger\nlogger = Logger().get_logger()\n</code></pre>"},{"location":"Python/python%20notes/#creating-decorator","title":"Creating decorator","text":"<pre><code>def logged_in_user(function) -&gt; Any:\n@wraps(function)\ndef get_logged_in_user(*args, **kwargs):\ncurrent_user = get_valid_current_user()\nif not current_user:\nreturn reject_access()\nreturn function(*args, **kwargs, current_user=current_user)\nreturn get_logged_in_user\n@app.post('/')\n@logged_in_user\ndef app_root_page(current_user):\n...\n</code></pre> <p>Decorator with params</p> <pre><code>def some_decorator(val) -&gt; Any:\ndef some_decorator_func(function) -&gt; Any:\n@wraps(function)\ndef wrapper(*args, **kwargs):\nsome_bool = val &gt; 10\n# do other stuff\nreturn function(*args, **kwargs, some_bool=some_bool)\nreturn wrapper\nreturn some_decorator_func\n</code></pre>"},{"location":"Python/python%20notes/#creating-batch-from-list","title":"creating batch from list","text":"<pre><code>def create_batch(self, obj_ls: list[\"ObjCls\"], batch_size: int):\nfor i in range(0, len(obj_ls), batch_size):\nyield obj_ls[i : i + batch_size]\n</code></pre>"},{"location":"Python/python%20notes/#list-filtering","title":"List filtering","text":"<p>Supposing we need to filter out the results from the below list of dictionaries</p> <pre><code>exam_results = [\n{\"name\": \"John\", \"score\": 70, \"subject\": \"German\"},\n{\"name\": \"Jane\", \"score\": 80, \"subject\": \"Maths\"},\n{\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"},\n{\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"},\n{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"},\n{\"name\": \"Theo\", \"score\": 40, \"subject\": \"Chemistry\"},\n]\n</code></pre> <p>To retrieve exam results where <code>score</code> is higher than 65:</p> <pre><code>filtered_result = list(filter(lambda x: x[\"score\"] &gt; 65, exam_results))\nprint(filtered_result)\n# [{\"name\": \"John\", \"score\": 70, \"subject\": \"German\"}, {\"name\": \"Jane\", \"score\": 80, \"subject\": \"Maths\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre> <p>To retrieve exam results where <code>score</code> is equal or higher than 50 and subject is either <code>Chemistry</code>, <code>French</code>, <code>Graphics</code>:</p> <pre><code>filtered_subject_results = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] in ([\"Chemistry\", \"French\", \"Graphics\"]), exam_results))\nprint(filtered_subject_results)\n# [{\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}, {\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}]\n</code></pre> <p>...and with <code>subject</code> in specific order using for-loop</p> <pre><code>filtered_subject_results = []\nfor subject_name in [\"Chemistry\", \"French\", \"Graphics\"]:\nresult = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] == subject_name, exam_results))\nfiltered_subject_results = filtered_subject_results + result\nprint(filtered_subject_results)\n# [{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}, {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre> <p>...or with <code>sorted</code></p> <pre><code>filtered_subject_results = list(filter(lambda x, subject=subject_name: x[\"score\"] &gt;= 50 and x[\"subject\"] in ([\"Chemistry\", \"French\", \"Graphics\"]), exam_results))\nfiltered_subject_results = sorted(filtered_subject_results, key=lambda x: (x[\"subject\"]))\nprint(filtered_subject_results)\n# [{\"name\": \"Alex\", \"score\": 50, \"subject\": \"Chemistry\"}, {\"name\": \"Mary\", \"score\": 60, \"subject\": \"French\"}, {\"name\": \"Sam\", \"score\": 75, \"subject\": \"Graphics\"}]\n</code></pre>"},{"location":"Python/python%20notes/#sort-groupby-itertools","title":"sort &amp; groupby (itertools)","text":"<p>Assuming we have a list of dictionaries of students needed to be grouped by year-class:</p> <pre><code>students_ls = [\n{\"name\": \"John\", \"email\": \"john@random_school.edu.uk\", \"year\": 1, \"class\": \"A\"},\n{\"name\": \"Jane\", \"email\": \"jane@random_school.edu.uk\", \"year\": 1, \"class\": \"B\"},\n{\"name\": \"Mary\", \"email\": \"mary@random_school.edu.uk\", \"year\": 2, \"class\": \"C\"},\n{\"name\": \"Alex\", \"email\": \"alex@random_school.edu.uk\", \"year\": 2, \"class\": \"A\"},\n{\"name\": \"Sam\", \"email\": \"sam@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n{\"name\": \"Hannah\", \"email\": \"hannah@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n{\"name\": \"Kim\", \"email\": \"kim@random_school.edu.uk\", \"year\": 3, \"class\": \"B\"},\n{\"name\": \"Ted\", \"email\": \"ted@random_school.edu.uk\", \"year\": 3, \"class\": \"A\"},\n]\n</code></pre> <p>First, we have to sorted the list by year and class as the documentation states that the list need to be sorted before applying <code>groupby</code></p> <pre><code>sorted_students_ls = sorted(students_ls, key=lambda x: (x[\"year\"], x[\"class\"]))\nprint(sorted_students_ls)\n# [\n#   {'class': 'A', 'email': 'john@random_school.edu.uk', 'name': 'John', 'year': 1},\n#   {'class': 'B', 'email': 'jane@random_school.edu.uk', 'name': 'Jane', 'year': 1},\n#   {'class': 'A', 'email': 'alex@random_school.edu.uk', 'name': 'Alex', 'year': 2},\n#   {'class': 'C', 'email': 'mary@random_school.edu.uk', 'name': 'Mary', 'year': 2},\n#   {'class': 'A', 'email': 'sam@random_school.edu.uk', 'name': 'Sam', 'year': 3},\n#   {'class': 'A', 'email': 'hannah@random_school.edu.uk', 'name': 'Hannah', 'year': 3},\n#   {'class': 'A', 'email': 'ted@random_school.edu.uk', 'name': 'Ted', 'year': 3},\n#   {'class': 'B', 'email': 'kim@random_school.edu.uk', 'name': 'Kim', 'year': 3}\n# ]\n</code></pre> <p>Then apply <code>groupby</code>:</p> <pre><code>grouped_data_single_line = {f\"{k[0]}-{k[1]}\": list(students) for k, students in itertools.groupby(sorted_students_ls, key=lambda x: (x[\"year\"], x[\"class\"]))}\nprint(grouped_data_single_line)\n# {\n#   \"1-A\": [{'class': 'A', 'email': 'john@random_school.edu.uk', 'name': 'John', 'year': 1}],\n#   \"1-B\": [{'class': 'B', 'email': 'jane@random_school.edu.uk', 'name': 'Jane', 'year': 1}],\n#   \"2-A\": [{'class': 'A', 'email': 'alex@random_school.edu.uk', 'name': 'Alex', 'year': 2}],\n#   \"2-C\": [{'class': 'C', 'email': 'mary@random_school.edu.uk', 'name': 'Mary', 'year': 2}],\n#   \"3-A\": [{'class': 'A', 'email': 'sam@random_school.edu.uk', 'name': 'Sam', 'year': 3},\n#           {'class': 'A', 'email': 'hannah@random_school.edu.uk', 'name': 'Hannah', 'year': 3},\n#           {'class': 'A', 'email': 'ted@random_school.edu.uk', 'name': 'Ted', 'year': 3}],\n#   \"3-B\": [{'class': 'B', 'email': 'kim@random_school.edu.uk', 'name': 'Kim', 'year': 3}]\n# }\n</code></pre> <p>or below if require data formatting</p> <pre><code>grouped_data = {}\nfor k, students in itertools.groupby(sorted_students_ls, key=lambda x: (x[\"year\"], x[\"class\"])):\ndata_key = f\"{k[0]}-{k[1]}\"\ngrouped_data[data_key] = [{\"name\": student[\"name\"], \"email\": student[\"email\"]} for student in list(students)]\nprint(grouped_data_single_line)\n# {\n#     '1-A': [{'email': 'john@random_school.edu.uk', 'name': 'John'}],\n#     '1-B': [{'email': 'jane@random_school.edu.uk', 'name': 'Jane'}],\n#     '2-A': [{'email': 'alex@random_school.edu.uk', 'name': 'Alex'}],\n#     '2-C': [{'email': 'mary@random_school.edu.uk', 'name': 'Mary'}],\n#     '3-A': [{'email': 'sam@random_school.edu.uk', 'name': 'Sam'},\n#             {'email': 'hannah@random_school.edu.uk', 'name': 'Hannah'},\n#             {'email': 'ted@random_school.edu.uk', 'name': 'Ted'}],\n#     '3-B': [{'email': 'kim@random_school.edu.uk', 'name': 'Kim'}]\n# }\n</code></pre>"},{"location":"Python/python%20notes/#dates-related","title":"Dates related","text":"<p>** Get last day of week from a given date **</p> <pre><code>from datetime import date, datetime, timedelta\ndef get_week_end(date_input: date):\n# use 6 for week ending on Sunday\nreturn date_input + timedelta(days=5 - date_input.weekday())\n</code></pre> <p>** Get last day of month from a given date **</p> <pre><code>import bisect\nimport calendar\nfrom datetime import date, datetime, timedelta\ndef get_month_end(date_input: date):\nlast_day_of_month = calendar.monthrange(date_input.year, date_input.month)[1]\nreturn date(date_input.year, date_input.month, last_day_of_month)\n</code></pre> <p>** Get start &amp; end of quarter from a given date **</p> <pre><code>import bisect\nimport calendar\nfrom datetime import date, datetime, timedelta\ndef get_quarter_end(date_input: date):\nquarter_ends = [date(date_input.year, month, 1) + timedelta(days=-1) for month in (4, 7, 10)]\nquarter_ends.append(date(date_input.year + 1, 1, 1) + timedelta(days=-1))\nidx = bisect.bisect_left(quarter_ends, date_input)\nreturn quarter_ends[idx]\ndef get_quarter_start(date_input: date):\nquarter_start = [date(date_input.year, month, 1) for month in (1, 4, 7, 10)]\nidx = bisect.bisect(quarter_start, date_input)\nreturn quarter_start[idx - 1]\n</code></pre>"},{"location":"Python/python%20notes/#user-authorisationpermission","title":"User authorisation/permission","text":"<p>An tiny user authorisation/permission python/flask class inspired by Ruby's authorization library Pundit</p> <pre><code>import inspect\nimport re\nfrom flask import g, request\nfrom path.to.user.model import User\n@staticmethod\ndef to_snake_case(obj_str):\nreg_match = re.compile(\"((?&lt;=[a-z0-9])[A-Z]|(?!^)(?&lt;!_)[A-Z](?=[a-z]))\")\nreturn reg_match.sub(r\"_\\1\", obj_str).lower()\nclass UserAuth:\ndef __init__(self):\nself._current_user = None\n@property\ndef current_user(self):\nif self._current_user is None:\n# can replace with g.current_user or method to get current user\nself._current_user = User.get(g.user_id)\nreturn self._current_user\n@classmethod\ndef get_policy_cls(cls, model_cls):\nmodel_cls_name = model_cls.__name__\nmodel_name_snake_case = to_snake_case(model_cls_name)\npolicy_path = f\"path.to.policies.{model_name_snake_case}_policy\"\npolicy_module = __import__(policy_path, fromlist=[f\"{model_cls_name}Policy\"])\npolicy_cls = getattr(policy_module, f\"{model_cls_name}Policy\")\nreturn policy_cls\n@classmethod\ndef authorised_action(cls, model_obj, action=None, *args, **kwargs):\nmodel_cls = cls.get_model_class(model_obj)\nmodel_policy = cls.get_policy_cls(model_cls)\naction = action or request.method.lower()\nreturn getattr(model_policy(self.current_user, model_obj), action)(*args, **kwargs)\n@classmethod\ndef authorised_scope(cls, model_obj, *args, **kwargs):\nmodel_cls = cls.get_model_class(model_obj)\nmodel_policy = cls.get_policy_cls(model_cls)\nreturn getattr(model_policy(self.current_user, model_cls), \"scope\")(*args, **kwargs)\n@classmethod\ndef get_model_class(cls, model_obj):\nif inspect.isclass(model_obj):\nreturn model_obj\nreturn model_obj.__class__\n</code></pre>"},{"location":"Python/python%20notes/#how-it-works","title":"How it works","text":"<p>Assuming we have a <code>User</code> class similar to the example below (with SQLAlchemy):</p> <pre><code># Path to SQLalchemy/Database config\nfrom app.setting.database import db\nclass UserRole(enum.Enum):\nREGISTER = 1\nADMIN = 2\nclass User(db.Model):\n# for dataclass, can do this instead of `__init__`\n# id: uuid.UUID\n# email: str\n# first_name: str\n# role: UserRole = UserRole.REGISTER\ndef __init__(self, email: str, name: str, role: int = UserRole.REGISTER) -&gt; None:\nself.email = email\nself.name = name\nself.role = role\n@classmethod\ndef create(cls, email: str, name: str, role: int = UserRole.REGISTER) -&gt; \"User\":\nuser = cls(email=email, name=name, role=role)\ndb.session.add(user)\ndb.session.commit()\nreturn user\n@classmethod\ndef get(cls, id: str) -&gt; \"User\":\nreturn cls.query.get(id)\n@classmethod\ndef all(cls) -&gt; list[\"User\"]:\nreturn cls.query.all()\ndef update(self, name: str, email:str) -&gt; None:\nself.name = name\nself.email = email\ndb.session.commit()\n@property\ndef is_admin(self) -&gt; bool:\nreturn self.role == UserRole.ADMIN\n</code></pre> <p>And <code>UserPolicy</code> - assuming only admin users can create, read, update and view all users, while registered users can only view their only user accounts:</p> <pre><code>class UserPolicy:\ndef __init__(self, user: User, user_obj: User) -&gt; None:\nself.user = user\nself.user_obj = user_obj\ndef create(self) -&gt; bool:\nreturn self.user.is_admin:\ndef update(self) -&gt; bool:\nreturn self.user.is_admin:\ndef get(self) -&gt; bool:\nreturn (self.user.id == self.user.id) or self.user.is_admin\ndef scope(self) -&gt; list[User]:\nif self.user.is_admin:\nreturn self.user_obj.all()\nreturn [self.user_obj.get(self.user.id)]\n</code></pre> <p>(Optional) And a custom exception class when current user not authorised to perform particular action(s) defined in the policy:</p> <pre><code>class PermissionRequired(BaseError):\ndef __init__(self):\nsuper().__init__(403, \"Permission denied\")\n</code></pre> <p>To check if the current user is authorised to perform a particular <code>User</code> action - in this example update, in a routing/view:</p> <pre><code>from path.to.user_auth import UserAuth\nfrom path.to.user.model import User\n@app.post('/users/&lt;user_id&gt;')\ndef update(user_id):\nuser = User.get(user_id)\nif UserAuth.authorize(user, \"update\"):\nreturn user\nreturn render_template(\"403.html\"), 403\n@app.post('/users')\ndef all():\nuser_records = UserAuth.authorised_scope(User)\nreturn jsonify(user_records)\n</code></pre> <p>or in a service class:</p> <pre><code>from path.to.user_auth import UserAuth\nfrom path.to.user.model import User\nclass UsersService:\ndef update(self, user_id: str, name: str, email: str) -&gt; dict[str, str]:\nuser = User.get(user_id)\nif not UserAuth.authorised_action(user, \"update\"):\nraise PermissionRequired()\nuser.update(name=name, email=email)\nreturn {\"message\": \"User successfully update\"}\n</code></pre> <p>The <code>UserAuth.authorize</code> takes two parameters:</p> <ul> <li>the model object (can be an instance or a class) you want to authorise the current user on</li> <li>the action or class method to authorise - in this case <code>update</code>. By default, if nothing passes as the parameter it will use the <code>request.method</code>, e.g. <code>post</code> in the example, and will require to define a <code>post</code> method in the policy:</li> </ul> <pre><code>class UserPolicy:\n...\ndef post(self) -&gt; bool:\nreturn self.user.is_admin:\n...\n</code></pre>"},{"location":"Python/python%20notes/#regex","title":"Regex","text":""},{"location":"Python/python%20notes/#remove-file-size-from-string","title":"Remove file size from string","text":"<pre><code>def remove_filesize_str(txt: str) -&gt; str:\nif not txt:\nreturn txt\npatt = r\"(^|\\s|\\,|\\:|\\/|\\(|\\[)?\\s*\\d+[\\.|\\,]?\\d*\\s*((G|T|M|K)((?!G|T|M|K)B|O)|bytes)\\s*(?=$|\\s|\\)|\\]|(?!\\w|\\d))(\\s|\\)|\\])?\"\ncleaned_txt = re.sub(patt, \" \", txt, flags=re.IGNORECASE)\ncleaned_txt = re.sub(r\"\\s\\s*\", \" \", cleaned_txt).strip()\nreturn cleaned_txt\n</code></pre>"},{"location":"Python/redis/","title":"Redis","text":""},{"location":"Python/redis/#cache-helper","title":"Cache helper","text":"<pre><code>from typing import TYPE_CHECKING, Any, Optional\nimport redis\nfrom app.config.config import config\nif TYPE_CHECKING:\nfrom datetime import datetime\nclass RedisCache:\ndef __init__(self, decode_responses=True) -&gt; None:\nhost = config.redis_db.host\nport = config.redis_db.port\nself.redis_client = redis.StrictRedis(host=host, port=port, decode_responses=decode_responses)\ndef set(self, key: str, value: Any) -&gt; Optional[bool]:\nself.redis_client.set(key, value)\ndef get(self, key: str) -&gt; Optional[bytes]:\nreturn self.redis_client.get(key)\ndef set_key_expire_at(self, key: str, expire_at: \"datetime\") -&gt; None:\nif self.redis_client.exists(key):\nself.redis_client.expireat(key, expire_at)\ndef get_keys_by_pattern(self, keys_pattern: str) -&gt; list:\nreturn [cache_key for cache_key in self.redis_client.scan_iter(keys_pattern)]\ndef delete_keys_by_pattern(self, keys_pattern: str) -&gt; list:\nfor cache_key in self.redis_client.scan_iter(keys_pattern):\nself.redis_client.delete(cache_key)\ndef flushall(self) -&gt; None:\nself.redis_client.flushall()\ndef exists(self, key) -&gt; int:\nreturn self.redis_client.exists(key)\ndef store_and_set_expire(self, cache_key: str, data: str, expire_at: \"datetime\") -&gt; None:\nself.set(cache_key, data)\nself.set_key_expire_at(cache_key, expire_at)\n</code></pre>"},{"location":"Python/redis/#events-streaming-with-redis-stream","title":"Events streaming with Redis Stream","text":"<p>Redis documentation on Stream Use of consumer groups Redis-py</p>"},{"location":"Python/redis/#publisher","title":"Publisher","text":"<pre><code>import json\nfrom concurrent import futures\nfrom datetime import datetime\nfrom typing import Any\nimport redis\nfrom logger import Logger\nclass EventsPublisher:\ndef __init__(self, event_stream_name: str) -&gt; None:\nself.logger = Logger().get_logger()\nself.event_stream_name = event_stream_name\nself._publisher = None\n@properity\ndef publisher(self):\nif self._publisher is None:\nself._publisher = redis.Redis(\nhost=config.event_publisher.host,\nport=config.event_publisher.port,\ndb=config.event_publisher.database,\n)\nreturn self._publisher\ndef batch_add(self, payloads: list, id_key=\"obj_id\") -&gt; tuple[list[str], list[str]]:\nsuccessful_entries_ids = []\nfailed_obj_ids = []\nwith futures.ThreadPoolExecutor() as executor:\ntasks = [executor.submit(self.add, payload, False, id_key) for payload in payloads]\nfor task in futures.as_completed(tasks):\ntry:\nentry_id, obj_id = task.result()\nif entry_id is not None:\nsuccessful_entries_ids.append(entry_id)\nelse:\nfailed_obj_ids.append(obj_id)\nexcept Exception:\nself.logger.exception(\"Error pushing data to stream\")\nreturn successful_entries_ids, failed_obj_ids\ndef add(self, payload: Any, raise_error=True, id_key=\"id\") -&gt; tuple[str, str]:\nobj_id = payload.get(id_key)\nobj_type = payload.get(\"obj_type\")\naction = payload.get(\"action\") # create / update etc\ntry:\njson_payload = json.dumps(payload)\nself.logger.info(\"Pushing %s data (%s) with ID %s to stream\", obj_type, action, obj_id)\nentry_id = self.publisher.xadd(self.event_stream_name, {\"payload\": json_payload})\nentry_id = entry_id.decode(\"utf-8\")\nself.logger.info(\n\"%s data (%s) with ID %s pushed - entry_id %s\", obj_type, action, obj_id, entry_id\n)\nreturn entry_id, obj_id\nexcept (\nTypeError,\nredis.exceptions.ConnectionError,\nredis.exceptions.TimeoutError,\nredis.exceptions.RedisError,\n) as e:\nmsg = f\"Failed to push {obj_type} data ({action}) with ID {obj_id} to stream: {str(e)}\"\nself.logger.info(msg)\nif raise_error:\nraise e\nreturn None, obj_id\ndef delete(self, msg_id: str) -&gt; dict[str, int]:\nstatus = self.publisher.xdel(self.event_stream_name, msg_id)\nreturn {msg_id: status}\ndef get_list(self, msg_count=10):\nevent_msgs = self.publisher.xread({self.event_stream_name: \"0-0\"}, count=msg_count)\ndata = {}\nif not event_msgs:\nreturn data\nfor message in event_msgs[0][1]:\nmessage_id = message[0].decode(\"utf-8\")\ntry:\nmessage_body = message[1]\npayload = message_body.get(b\"payload\")\npayload = payload.decode(\"utf-8\") if payload else {}\nexcept Exception:\npayload = {}\ndata[message_id] = payload\nreturn data\n</code></pre>"},{"location":"Python/redis/#consumer-one-consumer-no-additional-consumer-groups","title":"Consumer (One consumer - no additional consumer groups)","text":"<pre><code>import asyncio\nimport json\nfrom typing import Any\nfrom redis import asyncio as aioredis\nfrom path.to.config import config\nclass EventsConsumer:\ndef __init__(self, app) -&gt; None:\napp.app_context().push()\nself.logger = logger\nasync def handle_event(self, payload: dict) -&gt; None:\n# process the event message body\nawait process_event_message(payload)\nasync def main(self) -&gt; Any:\nconsumer_config = config.event_consumer\nconsumer_host = consumer_config.stream_host\nconsumer_port = consumer_config.stream_port\nconsumer_database = consumer_config.stream_database\nconsumer_stream = consumer_config.stream\nredis = await aioredis.from_url(\nf\"redis://{consumer_host}:{consumer_port}\",\ndb=consumer_database,\nencoding=\"utf8\",\ndecode_responses=True,\n)\nmessage_id = \"0-0\"\nwhile True:\nevents = await redis.xread({consumer_stream: message_id})\nif not events:\nawait asyncio.sleep(1)\nfor stream, message in events:\nmessage_id = message[0][0]\npayload = message[0][1][\"payload\"]\npayload = json.loads(payload)\nawait self.handle_event(payload)\nawait redis.xdel(stream, message_id)\n</code></pre> <p>To start consumer as thread:</p> <pre><code>def start_event_consumer():\nasyncio.run(EventsConsumer(app).main())\nworker = threading.Thread(target=start_event_consumer, args=[])\nworker.start()\n</code></pre>"},{"location":"Python/redis/#consumer-groups","title":"Consumer groups","text":"<p>Create consumer group(s) XGROUP CREATE:</p> <pre><code>redis_client.xgroup_create(name=stream_name, groupname=gname, id=0)\n</code></pre> <p>Read with consumer group(s) XREADGROUP:</p> <pre><code>redis_client.xreadgroup(groupname=group_1, consumername='consumer_a', streams={stream_key:'&gt;'})\n</code></pre> <p>Acknowledge messages XACK:</p> <pre><code>redis_client.xack(stream_name, groupname, message_id)\n</code></pre>"},{"location":"Python/sqlalchemy/","title":"Sqlalchemy","text":""},{"location":"Python/sqlalchemy/#intenum","title":"IntEnum","text":"<p>Storing the enum integer value to the database</p> <pre><code>from sqlalchemy import types\nclass IntEnum(types.TypeDecorator):\nimpl = Integer\ndef __init__(self, enumtype, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself._enumtype = enumtype\ndef process_bind_param(self, value, dialect):\nreturn value.value\ndef process_result_value(self, value, dialect):\nreturn self._enumtype(value)\n</code></pre> <pre><code>class Role(enum.Enum):\nREGISTERED = 1\nMODERATOR = 2\nADMIN = 3\nclass User(db.Model):\n__tablename__ = \"users\"\nid: uuid.UUID = db.Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\nname: str = db.Column(db.String(50), nullable=False)\nemail: str = db.Column(db.String(50), nullable=False)\nrole: int = db.Column(IntEnum(Role), nullable=False, default=1)\n</code></pre>"},{"location":"Python/sqlalchemy/#query-mixin","title":"Query mixin","text":"<pre><code>class QueryMixin:\n@classmethod\ndef get(cls, id):\nreturn cls.query.get(id)\n@classmethod\ndef _filters(cls, kwargs):\nreturn [getattr(cls, attr) == kwargs[attr] for attr in kwargs]\n@classmethod\ndef find_by(cls, **kwargs):\nfilters = cls._filters(kwargs)\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().first()\n@classmethod\ndef find_all(cls, **kwargs):\nfilters = cls._filters(kwargs)\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().all()\n@classmethod\ndef find_all_in(cls, **kwargs):\nfilters = [getattr(cls, attr).in_(kwargs[attr]) for attr in kwargs]\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().all()\n@classmethod\ndef find_all_not_in(cls, **kwargs):\nfilters = [getattr(cls, attr).not_in(kwargs[attr]) for attr in kwargs]\nreturn db.session.execute(db.select(cls).where(*filters)).scalars().all()\n@classmethod\ndef delete_if_exists(cls, **kwargs):\nfilters = cls._filters(kwargs)\ncls.query.where(*filters).delete()\ndb.session.commit()\ndef delete(self):\ndb.session.delete(self)\ndb.session.commit()\ndef to_dict(self):\nreturn {\ncolumn.name: getattr(self, column.name)\nif not isinstance(getattr(self, column.name), (datetime, date))\nelse getattr(self, column.name).isoformat()\nfor column in self.__table__.columns\n}\n</code></pre>"},{"location":"Python/sqlalchemy/#insertreturning","title":"Insert...returning","text":"<pre><code>    data = [\n{\"name\": \"Sally\", \"email\": \"sally@user.email\"},\n{\"name\": \"Jon\", \"email\": \"jon@user.email\"},\n{\"name\": \"Ken\", \"email\": \"ken@user.email\"},\n{\"name\": \"Jess\", \"email\": \"jess@user.email\"}\n]\nemployees = db.session.execute(insert(Employee).returning(Employee), data).all()\n</code></pre>"},{"location":"Python/sqlalchemy/#updatereturning","title":"Update...returning","text":"<pre><code>    employee_id = \"ec38f27b-79a2-4739-b96a-6bc2babcc2c9\"\nupdate_params = {\"name\": \"Ken\", \"email\": \"ken@another.email\"}\nupdate_stmt = update(Employee).where(Employee.id == employee_id).values(update_params).returning(Employee)\nemployees = db.session.execute(update_stmt).first()\n</code></pre>"},{"location":"Python/sqlalchemy/#update-multiple-records-where-the-where-conditons-is-different-for-each-record","title":"Update multiple records where the <code>WHERE</code> conditons is different for each record","text":"<pre><code>    data = [\n{\"employee_id\": \"443bd75a-3baa-4b17-a391-af00af9e3325\", \"name\": \"Sally\", \"email\": \"sally@user.email\", \"new_department\": \"Marketing\"},\n{\"employee_id\": \"52ca61aa-399d-4a0c-9482-6e0ec3ab4891\", \"name\": \"Jon\", \"email\": \"jon@user.email\", \"new_department\": \"IT\"},\n{\"employee_id\": \"ec38f27b-79a2-4739-b96a-6bc2babcc2c9\", \"name\": \"Ken\", \"email\": \"ken@user.email\", \"new_department\": \"Finance\"},\n{\"employee_id\": \"0d28cbbd-f073-4420-9f73-3dfc93c7696e\", \"name\": \"Jess\", \"email\": \"jess@user.email\", \"new_department\": \"Marketing\"},\n]\nupdate_stmt = (\nupdate(Employee)\n.where(Employee.id == bindparam(\"employee_id\"))\n.values(\n{\"department_id\": select(Department.id).where(Department.name == bindparam(\"new_department\")).scalar_subquery()}\n)\n)\ndb.session.execute(update_stmt, data)\n</code></pre>"},{"location":"Python/sqlalchemy/#nested-transaction","title":"Nested transaction","text":"<pre><code>    db.session.begin_nested()\n</code></pre>"},{"location":"Python/testing/","title":"Testing","text":""},{"location":"Python/testing/#pytest","title":"Pytest","text":""},{"location":"Python/testing/#testing-endpoints","title":"Testing endpoints","text":"<pre><code>from path.to.app_factory import app # or create_app()\n@pytest.fixture\ndef app(request):\nproject_app = _app # or create_app() factory\nctx = project_app.app_context()\nctx.push()\nyield project_app\nctx.pop()\n@pytest.fixture()\ndef test_client(app) -&gt; Flask:\nyield app.test_client()\n</code></pre> <pre><code>class TestClsNamePage:\n# Test get endpoint and assert content on page\ndef test_get_endpoint(self, test_client):\nresponse = test_client.get(\"/\")\nassert response.status_code == 200\npage_content = response.data.decode(\"utf-8\")\nassert \"Hello!\" in page_content\n# Test endpoint with headers\ndef test_endponint_with_headers(self, test_client):\nresponse = test_client.get(\"/\", headers={\"Auth-Token\": \"some-random-auth-token-values\"})\nassert response.status_code == 200\n# Test get endpoint and content on page after redirect\ndef test_get_endpoint(self, test_client):\nresponse = test_client.get(\"/redirect_to_another_page\", follow_redirects=True)\nassert response.status_code == 200\npage_content = response.data.decode(\"utf-8\")\nassert \"Redirected\" in page_content\n# Test get endpoint with JSON response\ndef test_get_endpoint(self, test_client):\n# assuming endpoint return {\"message\": \"Hello!\"}\nresponse = test_client.get(\"/json_response\")\nassert response.status_code == 200\ndata = json.loads(response.data)\nassert data[\"message\"] == \"Hello!\"\n# Test post endpoint - HTML form submission\ndef test_post_form_submit(self, test_client):\nresponse = test_client.post(\"/users\", data={\"name\": \"user\", \"email\": \"user@test.com\"})\nassert response.status_code == 200\n# Test post endpoint - HTML form submission with multi select\ndef test_post_multi_select_form_submit(self, test_client):\n# Remember to import ImmutableMultiDict: `from werkzeug.datastructures import ImmutableMultiDict`\nform = ImmutableMultiDict(\n[(\"user_id\", \"user_id_1\"), (\"user_id\", \"user_id_2\"), (\"user_id\", \"user_id_2\")]\n)\nresponse = test_client.post(\"/users/multi_select\", data=form)\nassert response.status_code == 200\n# Test post endpoint JSON params\ndef test_post_json_params(self, test_client):\nresponse = test_client.post(\"/users\", json={\"name\": \"user\", \"email\": \"user@test.com\"})\nassert response.status_code == 200\n</code></pre>"},{"location":"Python/testing/#parametrize-tests","title":"Parametrize tests","text":"<pre><code>import pytest\nclass TestClsName:\n@pytest.mark.parametrize(\"param_1, param_2, param3, expected_result\", [\n(test_1_param_1, test_1_param_2, test_1_param_3, test_1_expected_result),\n(test_2_param_1, test_2_param_2, test_2_param_3, test_2_expected_result),\n(test_3_param_1, test_3_param_2, test_3_param_3, test_3_expected_result),\n])\ndef test_some_test_with_parametrize(self, param_1, param_2, param3, expected_result):\nresult = method_to_test(param_1, param_2, param3)\nassert result == expected_result\n</code></pre>"},{"location":"Python/testing/#mockpatching","title":"Mock/patching","text":"<p>Mock method to return specific value</p> <pre><code>class TestClsName:\ndef test_method_with_patch(self):\nwith patch(\"path.to.class.ClassName.method_name\") as mocked_method:\nmocked_method.return_value = [1,2,3]\n</code></pre> <p>Mock method to raise exception</p> <pre><code>class TestClsName:\ndef test_method_with_mocked_exception(self):\nwith patch(\"path.to.class.ClassName.method_name\") as mocked_method:\nmocked_method.side_effect = Exception(\"error\")\n</code></pre> <p>Mock response with JSON response, status_code</p> <pre><code>class MockResponse:\ndef __init__(self, json_data, status_code):\nself.json_data = json_data\nself.status_code = status_code\ndef json(self):\nreturn self.json_data\nclass TestClsName:\ndef test_method_with_post_request(self):\nwith patch(\"app.model.ClsName.requests.post\") as mocked_request:\nmocked_request.return_value=MockResponse({\"message\": \"ta-da\", \"status\": \"success\"}, 200)\nresult = method_with_post_request()\n...\n</code></pre> <pre><code>class MockOpenAiResponse:\ndef __init__(self, json_data, status_code, headers=None, request=None, raise_for_status=None):\nself.json_data = json_data\nself.status_code = status_code\nself.headers = headers if headers else {}\nself.request = request if request else MagicMock()\nself.raise_for_status = raise_for_status if raise_for_status else MagicMock()\ndef json(self, db):\nreturn self.json_data\nwith patch(\"path.to.openai_client\") as mock_client:\nmock_client.return_value = MagicMock()\nmock_client.chat.completions.create.side_effect = openai.BadRequestError(\nmessage=\"error message\",\nresponse=MockOpenAiResponse({\"error\": \"error\"}, 400),\nbody={\"message\": \"BadRequestError\"},\n)\n</code></pre> <p>Mock response with raise_for_status</p> <pre><code>class TestClsName:\ndef test_method_with_post_request_raise_for_status(self):\nwith patch(\"app.model.ClsName.requests.post\") as mocked_request:\nmocked_status = Mock(status_code=500)\nmocked_status.raise_for_status = Mock(side_effect=requests.exceptions.RequestException(\"Error\"))\nmocked_request.return_value = mocked_status\nresult = method_with_post_request()\n...\n</code></pre>"},{"location":"Python/testing/#mock-aws-services-with-moto","title":"Mock AWS services with moto","text":"<p>For example, creating a S3 mock with moto</p> <pre><code>@pytest.fixture(scope=\"session\")\ndef aws_credentials():\n\"\"\"Mocked AWS Credentials for moto.\"\"\"\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\nos.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\nos.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\nos.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-1\"\n@pytest.fixture(autouse=False)\ndef s3_client(aws_credentials):\nwith mock_s3():\nconn = boto3.client(\"s3\", region_name=\"us-east-1\")\nyield conn\n</code></pre> <p>And in tests, use <code>s3_client</code> as fixture</p> <pre><code>@pytest.fixture\ndef s3_bucket(s3_client):\ns3_client.create_bucket(Bucket=bucket_name)\nyield\nclass TestS3Service:\ndef test_s3_bucket_list_objects(self, s3_client, s3_bucket):\nS3Service().list_objects()\n...\n</code></pre>"},{"location":"Python/testing/#monkeypatchingmocking-sqlalchemy-connection","title":"Monkeypatching/Mocking SQLAlchemy connection","text":"<p>For SQLAlchemy 2</p> <pre><code>@pytest.fixture(autouse=True, scope=\"function\")\ndef db(app, request):\n# https://github.com/pallets-eco/flask-sqlalchemy/issues/1171\nwith app.app_context():\nengines = _db.engines\nengine_cleanup = []\nfor key, engine in engines.items():\nconnection = engine.connect()\ntransaction = connection.begin_nested()\nengines[key] = connection\nengine_cleanup.append((key, engine, connection, transaction))\ntry:\nyield _db\nfinally:\nfor key, engine, connection, transaction in engine_cleanup:\ntransaction.rollback()\nconnection.close()\nengines[key] = engine\n</code></pre> <p>For SQLAlchemy 1.4</p> <pre><code>@pytest.fixture\ndef app(request):\napp = _app\nwith app.app_context():\nyield app\n@pytest.fixture\ndef db(app, request, monkeypatch):\nconnection = _db.engine.connect()\ntransaction = connection.begin()\n# https://github.com/pallets/flask-sqlalchemy/pull/249#issuecomment-628303481\nmonkeypatch.setattr(_db, \"get_engine\", lambda *args, **kwargs: connection)\ntry:\nyield _db\nfinally:\n_db.session.remove()\ntransaction.rollback()\nconnection.close()\n</code></pre> <p>And use <code>db</code> as fixture</p> <pre><code>@pytest.mark.usefixtures(\"db\")\nclass TestClsName:\n...\n</code></pre> <pre><code>def test_method_name(self, db):\n...\n</code></pre> <p>When testing database rollback use <code>db.session.begin_nested()</code> to begin a \"nested\" transaction/savepoint</p> <pre><code>class TestClsName:\ndef test_method_with_db_rollback(self, db):\ncreate_test_objects()\ndb.session.begin_nested()\nresult = method_with_db_rollback()\nassert result == expected_result\n</code></pre>"},{"location":"Python/testing/#assert-exception-raised","title":"Assert exception raised","text":"<pre><code>class TestUser:\ndef test_user_init_failed_on_missing_required_values(self):\nwith pytest.raises(TypeError) as error:\nUser()\nassert str(error.value) == \"__init__() missing 2 required positional argument: 'name', 'email'\"\n</code></pre>"},{"location":"Python/testing/#assert-parameters-pass-to-methodmock-method","title":"Assert parameters pass to method/mock method","text":"<p>Passing the actual method to <code>side_effect</code> will call the actual method instead of \"mocked\"</p> <pre><code>    with patch.object(ClsName, \"method_name\", side_effect=ClsName().method_name) as mocked_method:\nmocked_method.assert_called_with(\nparam_1=expected_param_1,\nparam_2=expected_param_2,\nparam_3=expected_param_3,\n)\n</code></pre>"},{"location":"Python/testing/#mock-results-of-repeatedmultiple-calls-to-the-same-method","title":"Mock results of repeated/multiple calls to the same method","text":"<p>For example, calling a method in a loop:</p> <pre><code>def loop_me():\nfor i in range(3):\nresult = do_something()\nprint(result)\n</code></pre> <p>To mock the results of repeated <code>do_something</code> calls</p> <pre><code>    with patch(\"path.to.do_something\", side_effect=(4, 5, 6)):\nloop_me()\n</code></pre>"},{"location":"Python/testing/#mock-property","title":"Mock property","text":"<pre><code>    class SomeClass:\n@property\ndef property_x():\nreturn something\n</code></pre> <pre><code>from mock import PropertyMock, patch\nwith patch(\"path.to.python.SomeClass.property_x\", new_callable=PropertyMock, return_value=some_value):\n# do something\n</code></pre> <p>or</p> <pre><code>from mock import PropertyMock, patch\nfrom app.domains.obj_cls import ObjCls\nwith patch.object(ObjCls, \"attr_or_property_name\", new_callable=PropertyMock) as mocked:\nmocked.return_value = some_value\n# do something\n</code></pre>"},{"location":"Python/text_extraction/","title":"Text extraction","text":""},{"location":"Python/text_extraction/#text-extraction","title":"Text extraction","text":""},{"location":"Python/text_extraction/#pdf-extraction-using-pymupdf4","title":"PDF extraction using PyMuPDF4","text":"<p>PyMuPDF4 | PyMuPDF4LLM</p> <p>WIP</p>"},{"location":"Python/text_extraction/#pdf-extraction-using-docling","title":"PDF extraction using Docling","text":"<p>Docling</p> <p>WIP</p> <p>Adobe PDF Extract API</p>"},{"location":"scribbles/clean_data_and_finance/","title":"Importance of Clean Data","text":"<p>Traditional sources of financial data are no longer enough for today\u2019s data-driven investment landscape. As machine learning and advanced analytics play a crucial role in portfolio construction and risk assessment, organisations are discovering a new source of competitive advantage: clean, high-quality data.</p> <p>Clean data is emerging as a fundamental differentiator in modern finance \u2014 it enables better decision-making, faster execution, stronger compliance, and more credible reporting.</p>"},{"location":"scribbles/clean_data_and_finance/#the-hidden-and-high-cost-of-dirty-data","title":"The Hidden (and High) Cost of Dirty Data","text":"<p>Inconsistent identifiers and formats, missing values, outdated reference data\u2026</p> <p>Poor data quality can easily (and silently) introduce risks into the investment process and skew portfolio models, generate misleading signals and suboptimal trades. Even worse, these issues often go undetected until damage has already been done \u2014 exposing the organisation to unnecessary risk.</p> <p>A study by IBM in 2016 shows that businesses and organisations in the US lose an estimated $3.1 trillion every year due to poor data quality.</p> <p>These are missed opportunities, false signals, and risk exposures that could have been avoided. A single error \u2014 e.g. a misclassified asset \u2014 can result in incorrect risk models and hedging strategies being implemented, leading to significant portfolio losses.</p>"},{"location":"scribbles/clean_data_and_finance/#trustworthy-data-as-a-business-advantage","title":"Trustworthy Data as a Business Advantage","text":""},{"location":"scribbles/clean_data_and_finance/#clean-data-smarter-faster-decisions","title":"Clean Data = Smarter, Faster Decisions","text":"<p>While poor data quality incurs hidden costs, clean data unlocks tangible business value and opportunities. Organisations that have mastered data quality are gaining significant advantage across multiple areas:</p> <ul> <li> <p>Improved decision-making   Accurate, real-time insights enable faster, more confident strategic choices. Companies with high data quality report faster decision-making cycles compared to their peers.</p> </li> <li> <p>Accelerated automation   Feeding reliable data into workflows reduces manual intervention and increases operational efficiency.</p> </li> <li> <p>Enhanced AI and machine learning performance   Clean data minimises bias and model drift. Superior data quality leads to better machine learning performance, directly translating into operational improvements and cost savings.</p> </li> <li> <p>Strengthened compliance and audit readiness   Particularly crucial in regulated sectors where data integrity is essential for meeting regulatory requirements and avoiding penalties.</p> </li> </ul>"},{"location":"scribbles/clean_data_and_finance/#ai-ready-data-for-portfolio-management","title":"AI-Ready Data for Portfolio Management","text":"<p>Modern portfolio management increasingly depends on artificial intelligence and machine learning algorithms that process vast amounts of data. However, these systems are only as good as the data they consume.</p> <p>Clean, AI-ready data offers key advantages that boost investment performance:</p> <ul> <li> <p>Real-time decision-making   Well-structured and well-governed data enables models to identify market opportunities and risks as they emerge.</p> </li> <li> <p>Improved model accuracy and reliability   High-quality training data produces more robust predictions \u2014 better stock selection, more accurate risk assessments, and improved market timing.</p> </li> <li> <p>Better risk management strategies   Reliable data infrastructure enables dynamic hedging strategies and risk monitoring:</p> </li> <li>Track thousands of risk factors continuously</li> <li>Detect early warning signals of market stress</li> <li>Identify concentration risks proactively</li> <li>Automatically adjust exposures to maintain optimal risk-return profiles</li> </ul>"},{"location":"scribbles/clean_data_and_finance/#building-the-foundation-for-clean-data-a-data-quality-culture","title":"Building the Foundation for Clean Data \u2014 a Data Quality Culture","text":"<p>Establishing clean data is not a one-time project. It requires more than technology \u2014 it demands a cultural shift across the organisation:</p> <ul> <li> <p>Standardised data definitions and taxonomies   Ensure consistency and interoperability across business units.</p> </li> <li> <p>Automated quality checks and anomaly detection   Catch and correct issues before they cascade through business processes.</p> </li> <li> <p>Dedicated data ownership   Create clear accountability for data quality.</p> </li> <li> <p>Modern data architecture   Ensure transparency, governance, and connectivity across the data ecosystem.</p> </li> <li> <p>Continuous monitoring and improvement   Treat data quality as an ongoing discipline, not a one-time fix.</p> </li> </ul> <p>This cultural transformation is critical for the success of any data quality initiative. It requires employees to understand the importance of clean data \u2014 and their role in upholding it.</p> <p>With AI and machine learning becoming integral to business operations, the importance of clean, reliable data will only grow. AI capabilities are no longer the differentiator \u2014 data quality is.</p>"},{"location":"scribbles/clean_data_and_finance/#final-thoughts","title":"Final Thoughts","text":"<p>Clean data is now a key competitive advantage in investment management. The ability to transform raw information into trustworthy, actionable insights is the foundation for future portfolio performance.</p> <p>Organisations that invest in clean data will outperform those that don\u2019t. Data integrity is no longer just a technical necessity \u2014 it\u2019s a strategic edge, a new source of alpha:</p> <ul> <li>Better insights</li> <li>Smarter, faster decision-making</li> <li>Enhanced operational efficiency</li> </ul> <p>Whether optimising portfolios, training AI models, or improve reporting \u2014 clean data is the differentiator.</p>"}]}